<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Training the First Model | cfh::blog</title>
<meta name="keywords" content="Python, PyTorch, ML">
<meta name="description" content="Now that we have plenty of training data, we can load it into PyTorch and
start training a model.
Loading the data
Since the binary file format we chose was so simple, it&rsquo;s rather straightforward to write a Dataset class which
reads it in:
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, Subset

class BinaryBezierDataset(Dataset):
    &#34;&#34;&#34;
    Loads Bezier triangle data from a binary file into memory once.
    Each record: 11 float32 coords, 32 uint8 bytes (packed 16x16 bitmap).
    &#34;&#34;&#34;
    def __init__(self, filename, device, input_dim=11):
        super().__init__()
        self.filename = filename
        self.input_dim = input_dim
        coords_bytes = input_dim * np.dtype(np.float32).itemsize  # 44
        record_bytes = coords_bytes &#43; 32       # 76 (coords &#43; 16x16 bitmap = 256 bits)

        # Calculate number of samples from file size
        file_size = os.path.getsize(filename)
        if file_size % record_bytes != 0:
            raise ValueError(f&#34;File size {file_size} not multiple of record size {record_bytes}&#34;)
        self.num_samples = file_size // record_bytes

        print(f&#34;Found {self.num_samples} samples in {filename}.&#34;)

        with open(filename, &#39;rb&#39;) as f:
            data = np.fromfile(f, dtype=np.uint8, count=file_size)

        data = data.reshape(self.num_samples, record_bytes)    # reshape into records

        # Extract coords (first 44 bytes = 11 floats)
        coords = data[:, :coords_bytes].view(np.float32).reshape(self.num_samples, self.input_dim)

        # Extract and unpack packed bitmaps (last 32 bytes)
        packed_bitmaps = data[:, coords_bytes:]
        unpacked_bits = np.unpackbits(packed_bitmaps, axis=1)  # (num_samples, 256)

        # The actual label is the maximum (0 or 1) over the bitmap bits
        outputs = np.max(unpacked_bits, axis=1)     # (num_samples,)

        # Convert to pytorch tensors and transfer to GPU if required
        self.x_tensor = torch.from_numpy(coords).float().to(device)   # (num_samples, 11)
        self.y_tensor = torch.from_numpy(outputs).float().to(device)  # (num_samples,)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return self.x_tensor[idx], self.y_tensor[idx]
So far, so good. We are in the convenient position that our entire dataset fits quite comfortably into RAM or VRAM,
so we just load the entire dataset at once, extract the 11 triangle coordinates, unpack the bitmap and take its maximum
to get a binary 0/1 label which tells us whether the triangle self-intersects. This is a pretty straightforward
DataSet which we can load into a nn.DataLoader with the desired batch size and shuffling enabled to feed a
standard PyTorch training loop. It&rsquo;s actually not very efficient to use it like this, but we&rsquo;ll get to that in a later post.">
<meta name="author" content="cfh">
<link rel="canonical" href="https://c-f-h.github.io/post/training-the-first-model/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.024295b3c968fbd469a11050839fd375a96747c3a5cff215e7f577090fe610f8.css" integrity="sha256-AkKVs8lo&#43;9RpoRBQg5/TdalnR8Olz/IV5/V3CQ/mEPg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://c-f-h.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://c-f-h.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://c-f-h.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://c-f-h.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://c-f-h.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://c-f-h.github.io/post/training-the-first-model/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><head>
  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
  
  
  
</head><meta property="og:url" content="https://c-f-h.github.io/post/training-the-first-model/">
  <meta property="og:site_name" content="cfh::blog">
  <meta property="og:title" content="Training the First Model">
  <meta property="og:description" content="Now that we have plenty of training data, we can load it into PyTorch and start training a model.
Loading the data Since the binary file format we chose was so simple, it’s rather straightforward to write a Dataset class which reads it in:
import numpy as np import torch from torch.utils.data import Dataset, DataLoader, Subset class BinaryBezierDataset(Dataset): &#34;&#34;&#34; Loads Bezier triangle data from a binary file into memory once. Each record: 11 float32 coords, 32 uint8 bytes (packed 16x16 bitmap). &#34;&#34;&#34; def __init__(self, filename, device, input_dim=11): super().__init__() self.filename = filename self.input_dim = input_dim coords_bytes = input_dim * np.dtype(np.float32).itemsize # 44 record_bytes = coords_bytes &#43; 32 # 76 (coords &#43; 16x16 bitmap = 256 bits) # Calculate number of samples from file size file_size = os.path.getsize(filename) if file_size % record_bytes != 0: raise ValueError(f&#34;File size {file_size} not multiple of record size {record_bytes}&#34;) self.num_samples = file_size // record_bytes print(f&#34;Found {self.num_samples} samples in {filename}.&#34;) with open(filename, &#39;rb&#39;) as f: data = np.fromfile(f, dtype=np.uint8, count=file_size) data = data.reshape(self.num_samples, record_bytes) # reshape into records # Extract coords (first 44 bytes = 11 floats) coords = data[:, :coords_bytes].view(np.float32).reshape(self.num_samples, self.input_dim) # Extract and unpack packed bitmaps (last 32 bytes) packed_bitmaps = data[:, coords_bytes:] unpacked_bits = np.unpackbits(packed_bitmaps, axis=1) # (num_samples, 256) # The actual label is the maximum (0 or 1) over the bitmap bits outputs = np.max(unpacked_bits, axis=1) # (num_samples,) # Convert to pytorch tensors and transfer to GPU if required self.x_tensor = torch.from_numpy(coords).float().to(device) # (num_samples, 11) self.y_tensor = torch.from_numpy(outputs).float().to(device) # (num_samples,) def __len__(self): return self.num_samples def __getitem__(self, idx): return self.x_tensor[idx], self.y_tensor[idx] So far, so good. We are in the convenient position that our entire dataset fits quite comfortably into RAM or VRAM, so we just load the entire dataset at once, extract the 11 triangle coordinates, unpack the bitmap and take its maximum to get a binary 0/1 label which tells us whether the triangle self-intersects. This is a pretty straightforward DataSet which we can load into a nn.DataLoader with the desired batch size and shuffling enabled to feed a standard PyTorch training loop. It’s actually not very efficient to use it like this, but we’ll get to that in a later post.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-04-10T20:29:11+02:00">
    <meta property="article:modified_time" content="2025-04-10T20:29:11+02:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="PyTorch">
    <meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training the First Model">
<meta name="twitter:description" content="Now that we have plenty of training data, we can load it into PyTorch and
start training a model.
Loading the data
Since the binary file format we chose was so simple, it&rsquo;s rather straightforward to write a Dataset class which
reads it in:
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, Subset

class BinaryBezierDataset(Dataset):
    &#34;&#34;&#34;
    Loads Bezier triangle data from a binary file into memory once.
    Each record: 11 float32 coords, 32 uint8 bytes (packed 16x16 bitmap).
    &#34;&#34;&#34;
    def __init__(self, filename, device, input_dim=11):
        super().__init__()
        self.filename = filename
        self.input_dim = input_dim
        coords_bytes = input_dim * np.dtype(np.float32).itemsize  # 44
        record_bytes = coords_bytes &#43; 32       # 76 (coords &#43; 16x16 bitmap = 256 bits)

        # Calculate number of samples from file size
        file_size = os.path.getsize(filename)
        if file_size % record_bytes != 0:
            raise ValueError(f&#34;File size {file_size} not multiple of record size {record_bytes}&#34;)
        self.num_samples = file_size // record_bytes

        print(f&#34;Found {self.num_samples} samples in {filename}.&#34;)

        with open(filename, &#39;rb&#39;) as f:
            data = np.fromfile(f, dtype=np.uint8, count=file_size)

        data = data.reshape(self.num_samples, record_bytes)    # reshape into records

        # Extract coords (first 44 bytes = 11 floats)
        coords = data[:, :coords_bytes].view(np.float32).reshape(self.num_samples, self.input_dim)

        # Extract and unpack packed bitmaps (last 32 bytes)
        packed_bitmaps = data[:, coords_bytes:]
        unpacked_bits = np.unpackbits(packed_bitmaps, axis=1)  # (num_samples, 256)

        # The actual label is the maximum (0 or 1) over the bitmap bits
        outputs = np.max(unpacked_bits, axis=1)     # (num_samples,)

        # Convert to pytorch tensors and transfer to GPU if required
        self.x_tensor = torch.from_numpy(coords).float().to(device)   # (num_samples, 11)
        self.y_tensor = torch.from_numpy(outputs).float().to(device)  # (num_samples,)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return self.x_tensor[idx], self.y_tensor[idx]
So far, so good. We are in the convenient position that our entire dataset fits quite comfortably into RAM or VRAM,
so we just load the entire dataset at once, extract the 11 triangle coordinates, unpack the bitmap and take its maximum
to get a binary 0/1 label which tells us whether the triangle self-intersects. This is a pretty straightforward
DataSet which we can load into a nn.DataLoader with the desired batch size and shuffling enabled to feed a
standard PyTorch training loop. It&rsquo;s actually not very efficient to use it like this, but we&rsquo;ll get to that in a later post.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://c-f-h.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Training the First Model",
      "item": "https://c-f-h.github.io/post/training-the-first-model/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Training the First Model",
  "name": "Training the First Model",
  "description": "Now that we have plenty of training data, we can load it into PyTorch and start training a model.\nLoading the data Since the binary file format we chose was so simple, it\u0026rsquo;s rather straightforward to write a Dataset class which reads it in:\nimport numpy as np import torch from torch.utils.data import Dataset, DataLoader, Subset class BinaryBezierDataset(Dataset): \u0026#34;\u0026#34;\u0026#34; Loads Bezier triangle data from a binary file into memory once. Each record: 11 float32 coords, 32 uint8 bytes (packed 16x16 bitmap). \u0026#34;\u0026#34;\u0026#34; def __init__(self, filename, device, input_dim=11): super().__init__() self.filename = filename self.input_dim = input_dim coords_bytes = input_dim * np.dtype(np.float32).itemsize # 44 record_bytes = coords_bytes + 32 # 76 (coords + 16x16 bitmap = 256 bits) # Calculate number of samples from file size file_size = os.path.getsize(filename) if file_size % record_bytes != 0: raise ValueError(f\u0026#34;File size {file_size} not multiple of record size {record_bytes}\u0026#34;) self.num_samples = file_size // record_bytes print(f\u0026#34;Found {self.num_samples} samples in {filename}.\u0026#34;) with open(filename, \u0026#39;rb\u0026#39;) as f: data = np.fromfile(f, dtype=np.uint8, count=file_size) data = data.reshape(self.num_samples, record_bytes) # reshape into records # Extract coords (first 44 bytes = 11 floats) coords = data[:, :coords_bytes].view(np.float32).reshape(self.num_samples, self.input_dim) # Extract and unpack packed bitmaps (last 32 bytes) packed_bitmaps = data[:, coords_bytes:] unpacked_bits = np.unpackbits(packed_bitmaps, axis=1) # (num_samples, 256) # The actual label is the maximum (0 or 1) over the bitmap bits outputs = np.max(unpacked_bits, axis=1) # (num_samples,) # Convert to pytorch tensors and transfer to GPU if required self.x_tensor = torch.from_numpy(coords).float().to(device) # (num_samples, 11) self.y_tensor = torch.from_numpy(outputs).float().to(device) # (num_samples,) def __len__(self): return self.num_samples def __getitem__(self, idx): return self.x_tensor[idx], self.y_tensor[idx] So far, so good. We are in the convenient position that our entire dataset fits quite comfortably into RAM or VRAM, so we just load the entire dataset at once, extract the 11 triangle coordinates, unpack the bitmap and take its maximum to get a binary 0/1 label which tells us whether the triangle self-intersects. This is a pretty straightforward DataSet which we can load into a nn.DataLoader with the desired batch size and shuffling enabled to feed a standard PyTorch training loop. It\u0026rsquo;s actually not very efficient to use it like this, but we\u0026rsquo;ll get to that in a later post.\n",
  "keywords": [
    "Python", "PyTorch", "ML"
  ],
  "articleBody": "Now that we have plenty of training data, we can load it into PyTorch and start training a model.\nLoading the data Since the binary file format we chose was so simple, it’s rather straightforward to write a Dataset class which reads it in:\nimport numpy as np import torch from torch.utils.data import Dataset, DataLoader, Subset class BinaryBezierDataset(Dataset): \"\"\" Loads Bezier triangle data from a binary file into memory once. Each record: 11 float32 coords, 32 uint8 bytes (packed 16x16 bitmap). \"\"\" def __init__(self, filename, device, input_dim=11): super().__init__() self.filename = filename self.input_dim = input_dim coords_bytes = input_dim * np.dtype(np.float32).itemsize # 44 record_bytes = coords_bytes + 32 # 76 (coords + 16x16 bitmap = 256 bits) # Calculate number of samples from file size file_size = os.path.getsize(filename) if file_size % record_bytes != 0: raise ValueError(f\"File size {file_size} not multiple of record size {record_bytes}\") self.num_samples = file_size // record_bytes print(f\"Found {self.num_samples} samples in {filename}.\") with open(filename, 'rb') as f: data = np.fromfile(f, dtype=np.uint8, count=file_size) data = data.reshape(self.num_samples, record_bytes) # reshape into records # Extract coords (first 44 bytes = 11 floats) coords = data[:, :coords_bytes].view(np.float32).reshape(self.num_samples, self.input_dim) # Extract and unpack packed bitmaps (last 32 bytes) packed_bitmaps = data[:, coords_bytes:] unpacked_bits = np.unpackbits(packed_bitmaps, axis=1) # (num_samples, 256) # The actual label is the maximum (0 or 1) over the bitmap bits outputs = np.max(unpacked_bits, axis=1) # (num_samples,) # Convert to pytorch tensors and transfer to GPU if required self.x_tensor = torch.from_numpy(coords).float().to(device) # (num_samples, 11) self.y_tensor = torch.from_numpy(outputs).float().to(device) # (num_samples,) def __len__(self): return self.num_samples def __getitem__(self, idx): return self.x_tensor[idx], self.y_tensor[idx] So far, so good. We are in the convenient position that our entire dataset fits quite comfortably into RAM or VRAM, so we just load the entire dataset at once, extract the 11 triangle coordinates, unpack the bitmap and take its maximum to get a binary 0/1 label which tells us whether the triangle self-intersects. This is a pretty straightforward DataSet which we can load into a nn.DataLoader with the desired batch size and shuffling enabled to feed a standard PyTorch training loop. It’s actually not very efficient to use it like this, but we’ll get to that in a later post.\nDesigning the model Quite frankly, my first attempt at designing a model was a bit overwrought. After a brainstorming session with Gemini 2.5 Pro, I drafted a model which would predict the 16x16 intersection curve heatmap described in the previous post instead of just the binary classifier. It consisted of some dense MLP layers of increasing size, until their output would be reshaped to 64 channels with a 4x4 resolution. Then a series of transposed convolution layers would successively upscale and reduce the number of channels until we arrived at a 1x16x16 image, which would be compared to the bitmap label via Dice loss.\nIt’s not that this design was a total failure; after training, it did make some reasonable predictions. But in the end it was slow to train relative to its number of parameters, the accuracy wasn’t particularly good, and it simply felt too complex for a first attempt.\nAt such times it’s a good idea to think back on the wisdom of Andrej Karpathy: don’t be a hero when it comes to model design. Our goal is to classify, so let’s stick to a single output neuron with a sigmoid activation. In between our input layer with 11 floats and the output, let’s just do the simplest thing that could possibly work: a sequence of \\(H\\) hidden, fully connected MLP layers with a fixed number \\(n\\) of neurons each.\nL i n e a r B a t c h N o r m R e L U D r o p o u t A single MLP layer for our first model.\nEach layer consists of a standard linear layer followed by batch normalization, ReLU activation and dropout. It’s generally considered not ideal to combine BatchNorm with dropout, but in our case, due to non-noisy data, being able to choose large batch sizes because our inputs are small, and using only very low dropout rates seems to make the combination work quite well.\nHere’s the Python function which instantiates our parameterized model:\ndef simple_mlp_model(inputdim, numlayers, numneurons, dropout_rate): layers = [ nn.Linear(inputdim, numneurons, bias=False), nn.BatchNorm1d(numneurons), nn.ReLU(), ] for k in range(numlayers): layers += [ nn.Linear(numneurons, numneurons, bias=False), nn.BatchNorm1d(numneurons), nn.ReLU(), nn.Dropout(p=dropout_rate), ] layers += [ nn.Linear(numneurons, 1), nn.Sigmoid() ] return nn.Sequential(*layers) Another useful little detail here is that we deactivate the bias terms on linear layers directly before BatchNorm layers since the latter provide their own bias term. It’s not a huge thing, but it does save a few parameters.\nOur loss function is binary_cross_entropy, and for training the model, we use the AdamW optimizer from PyTorch. For now we don’t use any weight decay because in my first tests it didn’t seem to be beneficial. The training loop is pretty standard PyTorch fare, and I won’t reproduce it here in its entirety.\nA first run After all that setup, it’s time to start a first training run. Let’s use 5 hidden layers with 256 neurons each; this model has around 333k parameters, so pretty quick to train. We use a learning rate of \\(10^{-3}\\) and a dropout rate of \\(p = 0.04\\). We train and validate using the 900,000 and 20,000 examples, respectively, described in the previous post.\nHere’s the plot of training and validation loss over the entire training run:\nWe use a simple kind of early stopping here based on keeping the model with the best validation accuracy and stopping if no improvements were made in the last 50 epochs. The best validation accuracy is achieved at epoch 188, with 95.44% accuracy on the validation set and a validation loss of 0.1149. That’s pretty nice for a first run! Granted, I chose these hyperparameters not randomly, but based on experience from some earlier experimental runs, so they already yield decent results.\nThe model is already overfitting here, but the validation loss isn’t suffering dramatically from it yet. We could try increasing the dropout rate or introducing some other kind of regularization, but we’ll get into a slightly more principled hyperparameter search later on.\nAlso note that the red dot marks the lowest validation loss, 0.1139, which happens earlier (epoch 111) but has a slightly lower accuracy of 95.12%.\nThis looks like a great starting point, and we will tweak various aspects of our setup from here on out.\n",
  "wordCount" : "1071",
  "inLanguage": "en",
  "datePublished": "2025-04-10T20:29:11+02:00",
  "dateModified": "2025-04-10T20:29:11+02:00",
  "author":{
    "@type": "Person",
    "name": "cfh"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://c-f-h.github.io/post/training-the-first-model/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "cfh::blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://c-f-h.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://c-f-h.github.io/" accesskey="h" title="cfh::blog (Alt + H)">cfh::blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://c-f-h.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://c-f-h.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://c-f-h.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://c-f-h.github.io/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Training the First Model
    </h1>
    <div class="post-meta"><span title='2025-04-10 20:29:11 +0200 CEST'>April 10, 2025</span>&nbsp;·&nbsp;cfh

</div>
  </header> 
  <div class="post-content"><p>Now that we have plenty of <a href="/post/preparing-the-data/">training data</a>, we can load it into PyTorch and
start training a model.</p>
<h2 id="loading-the-data">Loading the data<a hidden class="anchor" aria-hidden="true" href="#loading-the-data">#</a></h2>
<p>Since the binary file format we chose was so simple, it&rsquo;s rather straightforward to write a <code>Dataset</code> class which
reads it in:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Subset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BinaryBezierDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Loads Bezier triangle data from a binary file into memory once.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Each record: 11 float32 coords, 32 uint8 bytes (packed 16x16 bitmap).
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">11</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">filename</span> <span class="o">=</span> <span class="n">filename</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
</span></span><span class="line"><span class="cl">        <span class="n">coords_bytes</span> <span class="o">=</span> <span class="n">input_dim</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span>  <span class="c1"># 44</span>
</span></span><span class="line"><span class="cl">        <span class="n">record_bytes</span> <span class="o">=</span> <span class="n">coords_bytes</span> <span class="o">+</span> <span class="mi">32</span>       <span class="c1"># 76 (coords + 16x16 bitmap = 256 bits)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Calculate number of samples from file size</span>
</span></span><span class="line"><span class="cl">        <span class="n">file_size</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">file_size</span> <span class="o">%</span> <span class="n">record_bytes</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;File size </span><span class="si">{</span><span class="n">file_size</span><span class="si">}</span><span class="s2"> not multiple of record size </span><span class="si">{</span><span class="n">record_bytes</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">file_size</span> <span class="o">//</span> <span class="n">record_bytes</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Found </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="si">}</span><span class="s2"> samples in </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="n">file_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">record_bytes</span><span class="p">)</span>    <span class="c1"># reshape into records</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Extract coords (first 44 bytes = 11 floats)</span>
</span></span><span class="line"><span class="cl">        <span class="n">coords</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="n">coords_bytes</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Extract and unpack packed bitmaps (last 32 bytes)</span>
</span></span><span class="line"><span class="cl">        <span class="n">packed_bitmaps</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">coords_bytes</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="n">unpacked_bits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unpackbits</span><span class="p">(</span><span class="n">packed_bitmaps</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (num_samples, 256)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># The actual label is the maximum (0 or 1) over the bitmap bits</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">unpacked_bits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>     <span class="c1"># (num_samples,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Convert to pytorch tensors and transfer to GPU if required</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">coords</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>   <span class="c1"># (num_samples, 11)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># (num_samples,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_tensor</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_tensor</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span></code></pre></div><p>So far, so good. We are in the convenient position that our entire dataset fits quite comfortably into RAM or VRAM,
so we just load the entire dataset at once, extract the 11 triangle coordinates, unpack the bitmap and take its maximum
to get a binary 0/1 label which tells us whether the triangle self-intersects. This is a pretty straightforward
<code>DataSet</code> which we can load into a <code>nn.DataLoader</code> with the desired batch size and shuffling enabled to feed a
standard PyTorch training loop. It&rsquo;s actually not very efficient to use it like this, but we&rsquo;ll get to that in a later post.</p>
<h2 id="designing-the-model">Designing the model<a hidden class="anchor" aria-hidden="true" href="#designing-the-model">#</a></h2>
<p>Quite frankly, my first attempt at designing a model was a bit overwrought. After a brainstorming session with Gemini 2.5 Pro,
I drafted a model which would predict the 16x16 intersection curve heatmap described in the previous post instead of
just the binary classifier. It consisted of some dense MLP layers of increasing size, until their output would be reshaped
to 64 channels with a 4x4 resolution. Then a series of transposed convolution layers would successively upscale and reduce the
number of channels until we arrived at a 1x16x16 image, which would be compared to the bitmap label via Dice loss.</p>
<p>It&rsquo;s not that this design was a total failure; after training, it did make some reasonable predictions.
But in the end it was slow to train relative to its number of parameters, the accuracy wasn&rsquo;t particularly good, and it simply
felt too complex for a first attempt.</p>
<p>At such times it&rsquo;s a good idea to think back on the <a href="https://karpathy.github.io/2019/04/25/recipe/">wisdom of Andrej Karpathy</a>:
don&rsquo;t be a hero when it comes to model design. Our goal is to classify, so let&rsquo;s stick to a single output neuron with a
sigmoid activation. In between our input layer with 11 floats and the output, let&rsquo;s just do the simplest thing that could
possibly work: a sequence of \(H\) hidden, fully connected MLP layers with a fixed number \(n\) of neurons each.</p>




<figure id="diagram-2">
  
    <svg class="diagram" width="560" height="57"  xmlns="http://www.w3.org/2000/svg" version="1.1">
      <g transform='translate(8,16)'>
<path d='M 32,0 L 112,0' fill='none' stroke='currentColor'></path>
<path d='M 160,0 L 240,0' fill='none' stroke='currentColor'></path>
<path d='M 288,0 L 368,0' fill='none' stroke='currentColor'></path>
<path d='M 416,0 L 496,0' fill='none' stroke='currentColor'></path>
<path d='M 0,16 L 24,16' fill='none' stroke='currentColor'></path>
<path d='M 136,16 L 152,16' fill='none' stroke='currentColor'></path>
<path d='M 264,16 L 280,16' fill='none' stroke='currentColor'></path>
<path d='M 392,16 L 408,16' fill='none' stroke='currentColor'></path>
<path d='M 520,16 L 536,16' fill='none' stroke='currentColor'></path>
<path d='M 48,32 L 128,32' fill='none' stroke='currentColor'></path>
<path d='M 176,32 L 256,32' fill='none' stroke='currentColor'></path>
<path d='M 304,32 L 384,32' fill='none' stroke='currentColor'></path>
<path d='M 432,32 L 512,32' fill='none' stroke='currentColor'></path>
<path d='M 32,0 L 32,16' fill='none' stroke='currentColor'></path>
<path d='M 128,16 L 128,32' fill='none' stroke='currentColor'></path>
<path d='M 160,0 L 160,16' fill='none' stroke='currentColor'></path>
<path d='M 256,16 L 256,32' fill='none' stroke='currentColor'></path>
<path d='M 288,0 L 288,16' fill='none' stroke='currentColor'></path>
<path d='M 384,16 L 384,32' fill='none' stroke='currentColor'></path>
<path d='M 416,0 L 416,16' fill='none' stroke='currentColor'></path>
<path d='M 512,16 L 512,32' fill='none' stroke='currentColor'></path>
<polygon points='32.000000,16.000000 20.000000,10.400000 20.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 24.000000, 16.000000)'></polygon>
<polygon points='160.000000,16.000000 148.000000,10.400000 148.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 152.000000, 16.000000)'></polygon>
<polygon points='288.000000,16.000000 276.000000,10.400000 276.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 280.000000, 16.000000)'></polygon>
<polygon points='416.000000,16.000000 404.000000,10.400000 404.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 408.000000, 16.000000)'></polygon>
<polygon points='536.000000,16.000000 524.000000,10.400000 524.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 536.000000, 16.000000)'></polygon>
<path d='M 112,0 A 16,16 0 0,1 128,16' fill='none' stroke='currentColor'></path>
<path d='M 240,0 A 16,16 0 0,1 256,16' fill='none' stroke='currentColor'></path>
<path d='M 368,0 A 16,16 0 0,1 384,16' fill='none' stroke='currentColor'></path>
<path d='M 496,0 A 16,16 0 0,1 512,16' fill='none' stroke='currentColor'></path>
<path d='M 32,16 A 16,16 0 0,0 48,32' fill='none' stroke='currentColor'></path>
<path d='M 160,16 A 16,16 0 0,0 176,32' fill='none' stroke='currentColor'></path>
<path d='M 288,16 A 16,16 0 0,0 304,32' fill='none' stroke='currentColor'></path>
<path d='M 416,16 A 16,16 0 0,0 432,32' fill='none' stroke='currentColor'></path>
<circle cx='0' cy='16' r='6' stroke='currentColor' fill='currentColor'></circle>
<circle cx='544' cy='16' r='6' stroke='currentColor' fill='currentColor'></circle>
<text text-anchor='middle' x='56' y='20' fill='currentColor' style='font-size:1em'>L</text>
<text text-anchor='middle' x='64' y='20' fill='currentColor' style='font-size:1em'>i</text>
<text text-anchor='middle' x='72' y='20' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='80' y='20' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='88' y='20' fill='currentColor' style='font-size:1em'>a</text>
<text text-anchor='middle' x='96' y='20' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='176' y='20' fill='currentColor' style='font-size:1em'>B</text>
<text text-anchor='middle' x='184' y='20' fill='currentColor' style='font-size:1em'>a</text>
<text text-anchor='middle' x='192' y='20' fill='currentColor' style='font-size:1em'>t</text>
<text text-anchor='middle' x='200' y='20' fill='currentColor' style='font-size:1em'>c</text>
<text text-anchor='middle' x='208' y='20' fill='currentColor' style='font-size:1em'>h</text>
<text text-anchor='middle' x='216' y='20' fill='currentColor' style='font-size:1em'>N</text>
<text text-anchor='middle' x='224' y='20' fill='currentColor' style='font-size:1em'>o</text>
<text text-anchor='middle' x='232' y='20' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='240' y='20' fill='currentColor' style='font-size:1em'>m</text>
<text text-anchor='middle' x='320' y='20' fill='currentColor' style='font-size:1em'>R</text>
<text text-anchor='middle' x='328' y='20' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='336' y='20' fill='currentColor' style='font-size:1em'>L</text>
<text text-anchor='middle' x='344' y='20' fill='currentColor' style='font-size:1em'>U</text>
<text text-anchor='middle' x='440' y='20' fill='currentColor' style='font-size:1em'>D</text>
<text text-anchor='middle' x='448' y='20' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='456' y='20' fill='currentColor' style='font-size:1em'>o</text>
<text text-anchor='middle' x='464' y='20' fill='currentColor' style='font-size:1em'>p</text>
<text text-anchor='middle' x='472' y='20' fill='currentColor' style='font-size:1em'>o</text>
<text text-anchor='middle' x='480' y='20' fill='currentColor' style='font-size:1em'>u</text>
<text text-anchor='middle' x='488' y='20' fill='currentColor' style='font-size:1em'>t</text>
</g>

    </svg>
  
  <figcaption><p>A single MLP layer for our first model.</p></figcaption>
</figure><p>Each layer consists of a standard linear layer followed by batch normalization, ReLU activation and dropout.
It&rsquo;s generally considered not ideal to combine BatchNorm with dropout, but in our case, due to
non-noisy data, being able to choose large batch sizes because our inputs are small, and using only very low
dropout rates seems to make the combination work quite well.</p>
<p>Here&rsquo;s the Python function which instantiates our parameterized model:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">simple_mlp_model</span><span class="p">(</span><span class="n">inputdim</span><span class="p">,</span> <span class="n">numlayers</span><span class="p">,</span> <span class="n">numneurons</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inputdim</span><span class="p">,</span> <span class="n">numneurons</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">numneurons</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numlayers</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">numneurons</span><span class="p">,</span> <span class="n">numneurons</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">numneurons</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">numneurons</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</span></span></code></pre></div><p>Another useful little detail here is that we deactivate the bias terms on linear layers directly before
BatchNorm layers since the latter provide their own bias term. It&rsquo;s not a huge thing, but it does save a few parameters.</p>
<p>Our loss function is <code>binary_cross_entropy</code>, and for training the model, we use the <code>AdamW</code> optimizer
from PyTorch.
For now we don&rsquo;t use any weight decay because in my first tests it didn&rsquo;t seem to be beneficial.
The training loop is pretty standard PyTorch fare, and I won&rsquo;t reproduce it here in its entirety.</p>
<h2 id="a-first-run">A first run<a hidden class="anchor" aria-hidden="true" href="#a-first-run">#</a></h2>
<p>After all that setup, it&rsquo;s time to start a first training run. Let&rsquo;s use 5 hidden layers with 256 neurons each;
this model has around 333k parameters, so pretty quick to train.
We use a learning rate of \(10^{-3}\) and a dropout rate of \(p = 0.04\).
We train and validate using the 900,000 and 20,000 examples, respectively, described in the previous post.</p>
<p>Here&rsquo;s the plot of training and validation loss over the entire training run:</p>
<p><img alt="loss plot" loading="lazy" src="/post/training-the-first-model/lossplot.png"></p>
<p>We use a simple kind of early stopping here based on keeping the model with the best validation accuracy and stopping
if no improvements were made in the last 50 epochs.
The best validation accuracy is achieved at epoch 188, with <strong>95.44% accuracy</strong> on the validation set and a validation
loss of 0.1149. That&rsquo;s pretty nice for a first run! Granted, I chose these hyperparameters not randomly,
but based on experience from some earlier experimental runs, so they already yield decent results.</p>
<p>The model is already overfitting here, but the validation loss isn&rsquo;t suffering dramatically from it yet. We could try
increasing the dropout rate or introducing some other kind of regularization, but we&rsquo;ll get into a slightly more principled
hyperparameter search later on.</p>
<p>Also note that the red dot marks the lowest validation loss, 0.1139, which happens earlier (epoch 111)
but has a slightly lower accuracy of 95.12%.</p>
<p>This looks like a great starting point, and we will tweak various aspects of our setup from here on out.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://c-f-h.github.io/tags/python/">Python</a></li>
      <li><a href="https://c-f-h.github.io/tags/pytorch/">PyTorch</a></li>
      <li><a href="https://c-f-h.github.io/tags/ml/">ML</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://c-f-h.github.io/post/preparing-the-data/">
    <span class="title">« Prev</span>
    <br>
    <span>Preparing the Data</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://c-f-h.github.io/">cfh::blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
