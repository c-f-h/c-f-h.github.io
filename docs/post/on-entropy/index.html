<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>On Entropy | cfh::blog</title>
<meta name="keywords" content="ML, RL, Math">
<meta name="description" content="The last time, we ran our first self-play training loop
on a simple MLP model and observed catastrophic policy collapse. Let&rsquo;s first understand
some of the math behind what happened, and then how to combat it.
What is entropy?
Given a probability distribution \(p=(p_1,\ldots,p_C)\)
over a number of categories \(i=1,\ldots,C\), such as the distribution over the columns our
Connect 4 model outputs for a given board state, entropy measures the &ldquo;amount of
randomness&rdquo; and is defined as1">
<meta name="author" content="cfh">
<link rel="canonical" href="https://c-f-h.github.io/post/on-entropy/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.48368b920dcb61f046ba0ba3d41ca328a343973d6818b78739605e8749d21f3e.css" integrity="sha256-SDaLkg3LYfBGuguj1ByjKKNDlz1oGLeHOWBeh0nSHz4=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://c-f-h.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://c-f-h.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://c-f-h.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://c-f-h.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://c-f-h.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://c-f-h.github.io/post/on-entropy/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><head>
  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
  
  
  
</head><meta property="og:url" content="https://c-f-h.github.io/post/on-entropy/">
  <meta property="og:site_name" content="cfh::blog">
  <meta property="og:title" content="On Entropy">
  <meta property="og:description" content="The last time, we ran our first self-play training loop on a simple MLP model and observed catastrophic policy collapse. Let’s first understand some of the math behind what happened, and then how to combat it.
What is entropy? Given a probability distribution \(p=(p_1,\ldots,p_C)\) over a number of categories \(i=1,\ldots,C\), such as the distribution over the columns our Connect 4 model outputs for a given board state, entropy measures the “amount of randomness” and is defined as1">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-04-23T20:57:00+02:00">
    <meta property="article:modified_time" content="2025-04-23T20:57:00+02:00">
    <meta property="article:tag" content="ML">
    <meta property="article:tag" content="RL">
    <meta property="article:tag" content="Math">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="On Entropy">
<meta name="twitter:description" content="The last time, we ran our first self-play training loop
on a simple MLP model and observed catastrophic policy collapse. Let&rsquo;s first understand
some of the math behind what happened, and then how to combat it.
What is entropy?
Given a probability distribution \(p=(p_1,\ldots,p_C)\)
over a number of categories \(i=1,\ldots,C\), such as the distribution over the columns our
Connect 4 model outputs for a given board state, entropy measures the &ldquo;amount of
randomness&rdquo; and is defined as1">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://c-f-h.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "On Entropy",
      "item": "https://c-f-h.github.io/post/on-entropy/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "On Entropy",
  "name": "On Entropy",
  "description": "The last time, we ran our first self-play training loop on a simple MLP model and observed catastrophic policy collapse. Let\u0026rsquo;s first understand some of the math behind what happened, and then how to combat it.\nWhat is entropy? Given a probability distribution \\(p=(p_1,\\ldots,p_C)\\) over a number of categories \\(i=1,\\ldots,C\\), such as the distribution over the columns our Connect 4 model outputs for a given board state, entropy measures the \u0026ldquo;amount of randomness\u0026rdquo; and is defined as1\n",
  "keywords": [
    "ML", "RL", "Math"
  ],
  "articleBody": "The last time, we ran our first self-play training loop on a simple MLP model and observed catastrophic policy collapse. Let’s first understand some of the math behind what happened, and then how to combat it.\nWhat is entropy? Given a probability distribution \\(p=(p_1,\\ldots,p_C)\\) over a number of categories \\(i=1,\\ldots,C\\), such as the distribution over the columns our Connect 4 model outputs for a given board state, entropy measures the “amount of randomness” and is defined as1\n\\[ H(p) = -\\sum_{i=1}^{C} p_i \\log p_i. \\]A few examples will help here. Let’s first look at the “most random” distribution where every category has chance \\(p_i=1/C\\), then\n\\[ H(p) = -C \\frac1C \\log \\frac1C = \\log C. \\]For our case with \\(C=7\\) possible moves, this means the uniform random distribution has entropy \\(H(p) \\approx 1.946\\ldots\\)\nA high-entropy (left) and a low-entropy (right) probability distribution with three categories.\nOn the other hand, the “least random”, fully deterministic distribution which always chooses the same of the \\(C\\) labels with probability 100% has\n\\[ H(p) = -1\\log 1 = 0. \\]These are the two possible extremes: “fully random” with maximum entropy \\(\\log C\\) or fully deterministic with minimum entropy \\(0\\). All other distributions fall somewhere in between in terms of their entropy.\nIn the context of our policy model While playing a number of games, our model encountered a sequence of \\(n\\) board states \\((s_k)\\). In each state the model produced a probability distribution \\(p(s_k)\\) over the seven possible moves, each of which has an associated entropy \\(H(p(s_k))\\). (Recall that the model actually outputs raw logits, and we obtain the probabilities \\(p(s_k)\\) by applying the softmax operator.)\nThe plotted values in the entropy plot were simply the averaged values of the entropies in all board states it encountered since the last data point,\n\\[ \\frac 1n \\sum_k H(p(s_k)). \\]This lets us estimate the amount of randomness the model exhibits on average.\nNote that the model can have very different entropies in different situations: ideally, if there is a winning move on the board, the model should play it with very low entropy, but in the beginning of the game, it might play more randomly.\nReferring back to the entropy plot from the previous post, we observe that our model started out pretty close to a “fully random” state since at the beginning, the average entropy was above 1.8, which is close to the maximum possible entropy of 1.946… we saw above. This is due to the way PyTorch initializes our tensors by default. By the end of training, entropy had dropped to almost zero, which means the model was moving essentially deterministically in every situation.\nThis means that the model didn’t have any chance to learn from new states because the optimizer single-mindedly pushed the probability for the “known winning” moves higher and higher until no randomness was left. We need to find a way to keep entropy higher for longer so that our model has plenty of time to experiment. Once the model has a strong grasp on the required strategies and tactics, we can allow entropy to drop so that it can consistently play the strongest moves. This is referred to as the balance between exploration and exploitation in RL.\nLet’s examine some possible strategies for introducing additional randomness to our model.\nIdeas to increase entropy Dropout This might seem like it could work, but in practice it simply doesn’t do the job. It does introduce some extra variation during training, but in eval mode, which we use to sample new games, dropout is usually disabled, so it wouldn’t help at all to create a wider variety of games. We could keep the dropout enabled throughout, but that runs into its own issues since the probability distribution we use to sample games is then not the same we use during the RL policy update. This violates the so-called “on-policy” nature of the REINFORCE algorithm which requires that sampling is done using the same policy as training. Also, the added variance from dropout is probably not strong enough to encourage consistent exploration.\nTemperature Temperature is a standard way to increase the amount of randomness when sampling from a probabilistic model. The idea is simply to divide the raw logits from the model output by a parameter \\(T\\) before passing them through softmax. For \\(T\u003e1\\), this pushes the logits closer to 0 and therefore brings the probabilities closer to another. However, in addition to running into the off-policy problem again if we do this only during the sampling period, it also can’t fully deal with the situation where the model has already collapsed into a deterministic state since temperature doesn’t do much if the raw logits already have wildly different orders of magnitude.\n\\(\\varepsilon\\)-greedy sampling This is a common technique in other branches of RL. The idea is to introduce a probability epsilon, \\(\\varepsilon\u003e0\\), which describes the chance that during sampling a move, you completely ignore the model outputs and just sample a random move from a uniform distribution (i.e., every move with the same chance) instead.\nSadly, since we are using an on-policy algorithm, this doesn’t really work for us. It can force a move which the model wanted to play with probability \\(p_a(s)\\) essentially zero, which means that the policy gradient\n\\[ \\nabla \\ell = -\\frac{G \\nabla p_a(s)}{p_a(s)} \\]can become very large, destabilizing training. (Refer back to the post on the REINFORCE algorithm if you want to recall where this fomula came from.)\nEntropy regularization The idea here is to make sure entropy doesn’t drop too far in the first place. The way to do this is straightforward: if we want higher entropy, why don’t we simply tell the optimizer about it by including it in our loss function? In other words, we choose a parameter \\(\\beta\u003e0\\), called the entropy bonus, and modify our loss function by including an extra term which is just the scaled entropy \\(H(p(s))\\) at the current state \\(s\\),\n\\[ \\ell = -G \\log p_a(s) - \\beta H(p(s)). \\]Note the sign: since we minimize the loss, larger entropy \\(H\\) is rewarded more strongly if \\(\\beta\\) is larger. This additional term is minimal for a uniform distribution and penalizes all other distributions. This introduces a new hyperparameter \\(\\beta\\) which we need to tune carefully, but it’s worth the effort to avoid the collapse we observed.\nThis is the approach we’ll use going forward.\nBonus math: gradient computation What effect does the entropy bonus have on the loss gradient? We can easily compute by the product rule that\n\\[ \\frac{\\partial H(p)}{\\partial p_i} = -(1 + \\log p_i). \\]Therefore, by the chain rule, the gradient with respect to the model parameters is\n\\[ \\nabla H(p(s)) = -\\sum_{i=1}^C (1 + \\log p_i(s)) \\nabla p_i(s), \\]and our overall modified loss gradient becomes\n\\[ \\nabla \\ell = -G \\frac{\\nabla p_a(s)}{p_a(s)} + \\beta \\sum_{i=1}^C (1 + \\log p_i(s)) \\nabla p_i(s). \\]Our previous gradient was active only for the particular probability at entry \\(a\\) we wanted to reinforce or weaken with the return \\(G\\), but now we have contributions for all entries of the probability distribution. Since \\(1 + \\log p_i \u003c 0 \\Leftrightarrow p_i \u003c \\frac1e\\), where \\(1/e\\approx 0.368\\), probabilities lower than roughly 0.368 are pushed upwards, whereas probabilites above 0.368 are pushed downwards by this gradient contribution.\nThere are some additional mathematical subtleties since the probabilities have to remain normalized to 1, but the most important thing to take away is that this gradient term acts as a barrier against probabilities dropping to zero.\nOutlook All right, this was a bit more of a math-heavy post; next time we’ll implement the entropy bonus and run another experiment.\nNote on notation: We refer to an entire distribution by \\(p\\) and to an individual probability within it with a subscript, e.g., \\(p_i\\). ↩︎\n",
  "wordCount" : "1295",
  "inLanguage": "en",
  "datePublished": "2025-04-23T20:57:00+02:00",
  "dateModified": "2025-04-23T20:57:00+02:00",
  "author":{
    "@type": "Person",
    "name": "cfh"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://c-f-h.github.io/post/on-entropy/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "cfh::blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://c-f-h.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://c-f-h.github.io/" accesskey="h" title="cfh::blog (Alt + H)">cfh::blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://c-f-h.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://c-f-h.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://c-f-h.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://c-f-h.github.io/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      On Entropy
    </h1>



  
  
  

  
  

  
  
  
  
  

  
    
  
    
  
    
  
    
  
    
  
    
      

      
        
      

      
        
      

      

  
  
    <div class="series-info">
      <div class="series-name">
        Series: <a href="/categories/connect-zero">Connect-Zero</a>
        (Part 6 of 11)
      </div>
      <nav class="series-navigation">
        
          <a href="/post/policy-collapse/" title="Previous: A First Training Run and Policy Collapse">
            <span class="arrow">←</span> Previous
          </a>
        
        
          <a href="/post/entropy-regularization/" title="Next: Entropy Regularization">
            Next <span class="arrow">→</span>
          </a>
        
      </nav>
    </div>
  


    <div class="post-meta"><span title='2025-04-23 20:57:00 +0200 CEST'>April 23, 2025</span>&nbsp;·&nbsp;cfh

</div>
  </header> 
  <div class="post-content"><p><a href="/post/policy-collapse/">The last time</a>, we ran our first self-play training loop
on a simple MLP model and observed catastrophic policy collapse. Let&rsquo;s first understand
some of the math behind what happened, and then how to combat it.</p>
<h2 id="what-is-entropy">What is entropy?<a hidden class="anchor" aria-hidden="true" href="#what-is-entropy">#</a></h2>
<p>Given a probability distribution \(p=(p_1,\ldots,p_C)\)
over a number of categories \(i=1,\ldots,C\), such as the distribution over the columns our
Connect 4 model outputs for a given board state, entropy measures the &ldquo;amount of
randomness&rdquo; and is defined as<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
\[
    H(p) = -\sum_{i=1}^{C} p_i \log p_i.
\]<p>A few examples will help here. Let&rsquo;s first look at the &ldquo;most random&rdquo;
distribution where every category has chance \(p_i=1/C\), then</p>
\[
    H(p) = -C \frac1C \log \frac1C = \log C.
\]<p>For our case with \(C=7\) possible moves, this means the uniform random distribution has
entropy \(H(p) \approx 1.946\ldots\)</p>
<figure>
<svg width="300" height="200" xmlns="http://www.w3.org/2000/svg">
  <rect x="60" y="60" width="40" height="100" fill="#4CAF50" />
  <rect x="120" y="65" width="40" height="95" fill="#4CAF50" />
  <rect x="180" y="62" width="40" height="98" fill="#4CAF50" />
  <line x1="40" y1="160" x2="240" y2="160" stroke="#555" stroke-width="2" />
</svg>
<svg width="300" height="200" xmlns="http://www.w3.org/2000/svg">
  <rect x="60" y="147" width="40" height="13" fill="#4CAF50" />
  <rect x="120" y="0" width="40" height="160" fill="#4CAF50" />
  <rect x="180" y="138" width="40" height="22" fill="#4CAF50" />
  <line x1="40" y1="160" x2="240" y2="160" stroke="#555" stroke-width="2" />
</svg>
<figcaption>
<p>A high-entropy (left) and a low-entropy (right) probability distribution
  with three categories.</p>
</figcaption>
</figure>
<p>On the other hand, the &ldquo;least random&rdquo;, fully deterministic distribution which
always chooses the same of the \(C\) labels with probability 100% has</p>
\[
    H(p) = -1\log 1 = 0.
\]<p>These are the two possible extremes: &ldquo;fully random&rdquo; with maximum entropy \(\log C\) or fully deterministic with minimum entropy \(0\). All other distributions fall somewhere
in between in terms of their entropy.</p>
<h2 id="in-the-context-of-our-policy-model">In the context of our policy model<a hidden class="anchor" aria-hidden="true" href="#in-the-context-of-our-policy-model">#</a></h2>
<p>While playing a number of games, our model encountered a sequence of \(n\) board states
\((s_k)\). In each state the model
produced a probability distribution \(p(s_k)\) over the seven possible moves, each of
which has an associated entropy \(H(p(s_k))\). (Recall that the model actually outputs
raw logits, and we obtain the probabilities \(p(s_k)\) by applying the softmax operator.)</p>
<p>The plotted values in the entropy plot
were simply the averaged values of the entropies in all board states it encountered since
the last data point,</p>
\[
    \frac 1n \sum_k H(p(s_k)).
\]<p>This lets us estimate the amount of randomness the model exhibits on average.</p>
<p>Note that the
model can have very different entropies in different situations: ideally, if there is a
winning move on the board, the model should play it with very low entropy, but in the
beginning of the game, it might play more randomly.</p>
<p>Referring back to the entropy plot from
<a href="/post/policy-collapse/">the previous post</a>, we observe that our model started out
pretty close to a &ldquo;fully random&rdquo; state since at the beginning, the average entropy was
above 1.8, which is close to the maximum possible entropy of 1.946&hellip; we saw above. This is
due to the way PyTorch initializes our tensors by default. By the end of training,
entropy had dropped to almost zero, which means the model was moving essentially
deterministically in every situation.</p>
<p>This means that the model didn&rsquo;t have any chance to learn from new states because the
optimizer single-mindedly pushed the probability for the &ldquo;known winning&rdquo; moves higher and
higher until no randomness was left. We need to find a way to keep entropy higher for longer
so that our model has plenty of time to experiment. Once the model has a strong
grasp on the required strategies and tactics, we can allow entropy to drop so that it
can consistently play the strongest moves. This is referred to as the balance between
<strong>exploration and exploitation</strong> in RL.</p>
<p>Let&rsquo;s examine some possible strategies for introducing additional randomness to our model.</p>
<h2 id="ideas-to-increase-entropy">Ideas to increase entropy<a hidden class="anchor" aria-hidden="true" href="#ideas-to-increase-entropy">#</a></h2>
<h3 id="dropout">Dropout<a hidden class="anchor" aria-hidden="true" href="#dropout">#</a></h3>
<p>This might seem like it could work, but in practice it simply doesn&rsquo;t do the job. It does
introduce some extra variation during training, but in eval mode, which we use to sample
new games, dropout is usually disabled, so it wouldn&rsquo;t help at all to create a wider
variety of games. We could keep the dropout enabled throughout, but that runs into its own
issues since the probability distribution we use to sample games is then not the same we
use during the RL policy update. This violates the so-called <strong>&ldquo;on-policy&rdquo;</strong> nature of the
REINFORCE algorithm which requires that sampling is done using the same policy as training.
Also, the added variance from dropout is probably not strong enough to encourage consistent
exploration.</p>
<h3 id="temperature">Temperature<a hidden class="anchor" aria-hidden="true" href="#temperature">#</a></h3>
<p>Temperature is a standard way to increase the amount of randomness when sampling from a probabilistic model. The idea is simply to divide the raw logits from the model output by a parameter \(T\) before passing them through softmax. For \(T>1\), this pushes the logits closer to 0 and therefore brings the probabilities closer to another. However, in addition to running into the off-policy problem again if we do this only during the sampling period, it also can&rsquo;t fully deal with the situation where the model has already collapsed into a deterministic state since temperature doesn&rsquo;t do much if the raw logits already have wildly different orders of magnitude.</p>
<h3 id="-greedy-sampling">\(\varepsilon\)-greedy sampling<a hidden class="anchor" aria-hidden="true" href="#-greedy-sampling">#</a></h3>
<p>This is a common technique in other branches of RL. The idea is to introduce a probability epsilon, \(\varepsilon>0\), which describes the chance that during sampling a move, you completely ignore the model outputs and just sample a random move from a uniform distribution (i.e., every move with the same chance) instead.</p>
<p>Sadly, since we are using an on-policy algorithm, this doesn&rsquo;t really work for us. It can force a move which the model wanted to play with probability \(p_a(s)\) essentially zero, which means that the policy gradient</p>
\[
  \nabla \ell = -\frac{G \nabla p_a(s)}{p_a(s)}
\]<p>can become very large, destabilizing training. (Refer back to the post on the
<a href="/post/the-reinforce-algorithm/">REINFORCE algorithm</a>
if you want to recall where this fomula came from.)</p>
<h3 id="entropy-regularization">Entropy regularization<a hidden class="anchor" aria-hidden="true" href="#entropy-regularization">#</a></h3>
<p>The idea here is to make sure entropy doesn&rsquo;t drop too far in the first
place. The way to do this is straightforward: if we want higher entropy, why
don&rsquo;t we simply tell the optimizer about it by including it in our loss function? In other
words, we choose a parameter \(\beta>0\), called the entropy bonus, and modify our loss
function by including an extra term which is just the scaled entropy \(H(p(s))\) at the
current state \(s\),</p>
\[
    \ell = -G \log p_a(s) - \beta H(p(s)).
\]<p>Note the sign: since we minimize the loss, larger entropy \(H\) is rewarded more strongly
if \(\beta\) is larger.
This additional term is minimal for a uniform distribution and penalizes all
other distributions.
This introduces a new hyperparameter \(\beta\) which we need to
tune carefully, but it&rsquo;s worth the effort to avoid the collapse we observed.</p>
<p>This is the approach we&rsquo;ll use going forward.</p>
<h2 id="bonus-math-gradient-computation">Bonus math: gradient computation<a hidden class="anchor" aria-hidden="true" href="#bonus-math-gradient-computation">#</a></h2>
<p>What effect does the entropy bonus have on the loss gradient? We can easily compute by the
product rule that</p>
\[
    \frac{\partial H(p)}{\partial p_i} = -(1 + \log p_i).
\]<p>Therefore, by the chain rule, the gradient with respect to the model parameters is</p>
\[
    \nabla H(p(s)) = -\sum_{i=1}^C (1 + \log p_i(s)) \nabla p_i(s),
\]<p>and our overall modified loss gradient becomes</p>
\[
    \nabla \ell =
    -G \frac{\nabla p_a(s)}{p_a(s)} + \beta \sum_{i=1}^C (1 + \log p_i(s)) \nabla p_i(s).
\]<p>Our previous gradient was active only for the particular probability at entry \(a\) we
wanted to reinforce or weaken with the return \(G\),
but now we have contributions for all entries of the
probability distribution. Since \(1 + \log p_i < 0 \Leftrightarrow p_i < \frac1e\), where
\(1/e\approx 0.368\), probabilities lower than roughly 0.368 are pushed upwards, whereas
probabilites above 0.368 are pushed downwards by this gradient contribution.</p>
<p>There are some
additional mathematical subtleties since the probabilities have to remain normalized to 1,
but the most important thing to take away is that this gradient term acts as a barrier
against probabilities dropping to zero.</p>
<h2 id="outlook">Outlook<a hidden class="anchor" aria-hidden="true" href="#outlook">#</a></h2>
<p>All right, this was a bit more of a math-heavy post; <a href="/post/entropy-regularization/">next time</a> we&rsquo;ll implement the
entropy bonus and run another experiment.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Note on notation: We refer to an entire distribution by \(p\) and to an individual
probability within it with a subscript, e.g., \(p_i\).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://c-f-h.github.io/tags/ml/">ML</a></li>
      <li><a href="https://c-f-h.github.io/tags/rl/">RL</a></li>
      <li><a href="https://c-f-h.github.io/tags/math/">Math</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://c-f-h.github.io/post/policy-collapse/">
    <span class="title">« Prev</span>
    <br>
    <span>A First Training Run and Policy Collapse</span>
  </a>
  <a class="next" href="https://c-f-h.github.io/post/entropy-regularization/">
    <span class="title">Next »</span>
    <br>
    <span>Entropy Regularization</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://c-f-h.github.io/">cfh::blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
