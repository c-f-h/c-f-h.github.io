<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The REINFORCE Algorithm | cfh::blog</title>
<meta name="keywords" content="Python, PyTorch, ML, RL, Math">
<meta name="description" content="Let&rsquo;s say we have a Connect 4-playing model and we let it
play a couple of games.
(We haven&rsquo;t really talked about model architecture until now, so for now just imagine a
simple multilayer perceptron with a few hidden layers which outputs 7 raw logits,
as discussed in the previous post.)
As it goes in life, our model wins some and loses some. How do we make it
actually learn from its experiences? How does the magic happen?">
<meta name="author" content="cfh">
<link rel="canonical" href="https://c-f-h.github.io/post/the-reinforce-algorithm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.48368b920dcb61f046ba0ba3d41ca328a343973d6818b78739605e8749d21f3e.css" integrity="sha256-SDaLkg3LYfBGuguj1ByjKKNDlz1oGLeHOWBeh0nSHz4=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://c-f-h.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://c-f-h.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://c-f-h.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://c-f-h.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://c-f-h.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://c-f-h.github.io/post/the-reinforce-algorithm/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><head>
  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
  
  
  
</head><meta property="og:url" content="https://c-f-h.github.io/post/the-reinforce-algorithm/">
  <meta property="og:site_name" content="cfh::blog">
  <meta property="og:title" content="The REINFORCE Algorithm">
  <meta property="og:description" content="Let’s say we have a Connect 4-playing model and we let it play a couple of games. (We haven’t really talked about model architecture until now, so for now just imagine a simple multilayer perceptron with a few hidden layers which outputs 7 raw logits, as discussed in the previous post.)
As it goes in life, our model wins some and loses some. How do we make it actually learn from its experiences? How does the magic happen?">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-04-20T20:29:21+02:00">
    <meta property="article:modified_time" content="2025-04-20T20:29:21+02:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="PyTorch">
    <meta property="article:tag" content="ML">
    <meta property="article:tag" content="RL">
    <meta property="article:tag" content="Math">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The REINFORCE Algorithm">
<meta name="twitter:description" content="Let&rsquo;s say we have a Connect 4-playing model and we let it
play a couple of games.
(We haven&rsquo;t really talked about model architecture until now, so for now just imagine a
simple multilayer perceptron with a few hidden layers which outputs 7 raw logits,
as discussed in the previous post.)
As it goes in life, our model wins some and loses some. How do we make it
actually learn from its experiences? How does the magic happen?">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://c-f-h.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The REINFORCE Algorithm",
      "item": "https://c-f-h.github.io/post/the-reinforce-algorithm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The REINFORCE Algorithm",
  "name": "The REINFORCE Algorithm",
  "description": "Let\u0026rsquo;s say we have a Connect 4-playing model and we let it play a couple of games. (We haven\u0026rsquo;t really talked about model architecture until now, so for now just imagine a simple multilayer perceptron with a few hidden layers which outputs 7 raw logits, as discussed in the previous post.)\nAs it goes in life, our model wins some and loses some. How do we make it actually learn from its experiences? How does the magic happen?\n",
  "keywords": [
    "Python", "PyTorch", "ML", "RL", "Math"
  ],
  "articleBody": "Let’s say we have a Connect 4-playing model and we let it play a couple of games. (We haven’t really talked about model architecture until now, so for now just imagine a simple multilayer perceptron with a few hidden layers which outputs 7 raw logits, as discussed in the previous post.)\nAs it goes in life, our model wins some and loses some. How do we make it actually learn from its experiences? How does the magic happen?\nThe REINFORCE algorithm is one of the basic workhorses of reinforcement learning, and we’ll start from there. It’s an example of a policy gradient method, which essentially means that it applies the gradient descent techniques we know and love from basic ML to improve a policy network. The good news is that it’s fundamentally a simple method.\nThe basic idea We want to reward the model for good moves and punish it for bad ones, but the issue is that at the moment we make a move, it’s usually not obvious yet if it was good or bad. So for now let’s just say that moves that ultimately led to a win were probably good, and moves that led to a loss were probably bad.\nIf the model found itself in board state \\(s\\) and took an action \\(a\\), and it turned out to be a winning move, we want the model to do more of that. The standard deep learning way of achieving that is computing the gradient \\(\\nabla p_a(s)\\) (with respect to the model weights) of the probability \\(p_a(s)\\) of choosing \\(a\\), and then taking a gradient step of the model weights in that direction; this naturally pushes \\(p_a(s)\\) higher.\nHow big should our step be? We could come up with all kinds of heuristics for that, but why not simply stick to a formula that we already know works well? If the model currently outputs probabilities \\( p_i(s) \\) for the seven possible actions \\( i = 0, \\ldots, 6 \\) in state \\(s\\) and the one we want it to actually take is \\(a\\), that’s nothing else but a standard classification problem. The standard loss function for that is cross entropy loss with the simple formula\n\\[\r\\ell = -\\log p_a(s).\r\\]As an aside, if we compute the gradient of that, it’s just\n\\[\r\\nabla\\ell = - \\frac{\\nabla p_a(s)}{p_a(s)}\r\\]and that kind of makes intuitive sense: if the probability \\(p_a(s)\\) we want to be high is already close to 1, we take a smaller step, but if \\(p_a(s)\\) is small, the step is larger.\nWe have an additional bit of information, and that is the return we got from making move \\(a\\); it’s a real number \\(G \\in [-1,1] \\) representing the cumulative reward for an action, adjusted for future outcomes. We’ll discuss it further below.\nIf we simply multiply our loss with the return, we get the complete REINFORCE policy loss function for a single sample,\n\\[\r\\ell = -G \\log p_a(s).\r\\]It’s clear that it takes larger steps for moves that gave us a higher return, and if the move led to a loss and therefore \\(G\u003c0\\), we actually take a gradient step in the opposite direction, which is exactly what we want in order to discourage taking this action in the future.\nAssigning returns If we play an entire game (often called a “trajectory” in RL) with \\(n\\) moves, we end up with sequences\n\\[\r(s_t), \\quad (a_t), \\qquad t=1,\\ldots,n\r\\]of board states and actions we took in those states. To obtain the complete REINFORCE policy loss, we can simply sum up the single-state loss function shown above. But we do need returns \\(G_t \\in [-1,1]\\) for each of the taken actions. How should we assign them?\nWe could simply set \\(G_t = 1\\) for all moves in won games and \\(G_t = -1\\) for all moves in lost games, but that seems a bit overconfident. Maybe we made a few suboptimal moves early on but still managed to pull off the win, and we don’t want to reward those early moves with the same weight.\nOn the other hand, if we set \\(G_n=\\pm1\\) for the final winning or losing move only and \\(G_t=0\\) for all previous moves, we don’t learn anything at all about moves that brought us into an ultimately advantageous position. The usual compromise in REINFORCE is to choose a reward discount rate \\(\\gamma\u003c1\\) and compute the returns as\n\\[\rG_t = R \\gamma^{n-t}, \\qquad t=1,\\ldots,n\r\\]where \\(R\\) is 1 for a win and -1 for a loss; common choices are \\(\\gamma\\in[0.9,0.99]\\). This just means that the final move gets the full reward \\(R\\), and the previous moves get weighted less and less the earlier in the game they occurred. This is called discounted returns.\nOf course, if we played an entire batch of games, we can simply assign the returns for each game individually and then throw them all together into a full batch of states, actions, and returns. Our final batched policy loss function is then simply\n\\[\r\\ell = - \\sum_{t=1}^n G_t \\log p_{a_t}(s_t).\r\\]Implementation Once we understood the formula, it’s actually not too hard to implement in PyTorch. In practice we don’t take the gradient step directly, but pass it to an optimizer like Adam, which usually performs better than standard SGD. Here’s a basic implementation:\ndef update_policy( model: nn.Module, optimizer: optim.Optimizer, states: torch.Tensor, # (B, 6, 7) - all board states actions: torch.Tensor, # (B,) - actions taken in those states returns: torch.Tensor # (B,) - discounted returns for the actions ): \"\"\"Update the policy model using the REINFORCE algorithm.\"\"\" model.train() # compute model logit outputs for the batch logits = model(states) # (B, 7) # mask out illegal moves and compute log probs of all actions masked_logits = mask_invalid_moves_batch(states, logits) # (B, 7) log_probs = F.log_softmax(masked_logits, dim=1) # (B, 7) # select log probs of actually taken actions: (B, 7) -\u003e (B, 1) -\u003e (B,) log_probs_taken = torch.gather(log_probs, dim=1, index=actions.unsqueeze(1)).squeeze(1) # calculate REINFORCE loss = - sum(G_t * log p_(a_t)(s_t), t) policy_loss = -(returns * log_probs_taken).sum() # perform gradient update optimizer.zero_grad() policy_loss.backward() optimizer.step() There’s one new function here, mask_invalid_moves_batch, which performs the masking operation for illegal moves described in the previous post for an entire batch; simple stuff.\nIt’s also worth pointing out the use of the torch.gather function: we use it to pick out from the tensor of all seven log-probabilities per sample precisely the one corresponding to the action actually taken by indexing along the second axis with the actions tensor. The tensor being used as an index needs to have the same number of dimensions as the original tensor, which is why we need to call actions.unsqueeze(1) to add a second singleton axis to actions.\nWith the REINFORCE algorithm implemented, we now have everything we need to start training some models.\n",
  "wordCount" : "1138",
  "inLanguage": "en",
  "datePublished": "2025-04-20T20:29:21+02:00",
  "dateModified": "2025-04-20T20:29:21+02:00",
  "author":{
    "@type": "Person",
    "name": "cfh"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://c-f-h.github.io/post/the-reinforce-algorithm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "cfh::blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://c-f-h.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://c-f-h.github.io/" accesskey="h" title="cfh::blog (Alt + H)">cfh::blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://c-f-h.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://c-f-h.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://c-f-h.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://c-f-h.github.io/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      The REINFORCE Algorithm
    </h1>



  
  
  

  
  

  
  
  
  
  

  
    
  
    
  
    
  
    
      

      
        
      

      
        
      

      

  
  
    <div class="series-info">
      <div class="series-name">
        Series: <a href="/categories/connect-zero">Connect-Zero</a>
        (Part 4 of 11)
      </div>
      <nav class="series-navigation">
        
          <a href="/post/basic-setup-and-play/" title="Previous: Basic Setup and Play">
            <span class="arrow">←</span> Previous
          </a>
        
        
          <a href="/post/policy-collapse/" title="Next: A First Training Run and Policy Collapse">
            Next <span class="arrow">→</span>
          </a>
        
      </nav>
    </div>
  


    <div class="post-meta"><span title='2025-04-20 20:29:21 +0200 CEST'>April 20, 2025</span>&nbsp;·&nbsp;cfh

</div>
  </header> 
  <div class="post-content"><p>Let&rsquo;s say we have a Connect 4-playing model and we let it
<a href="/post/basic-setup-and-play/">play a couple of games</a>.
(We haven&rsquo;t really talked about model architecture until now, so for now just imagine a
simple multilayer perceptron with a few hidden layers which outputs 7 raw logits,
as discussed in the previous post.)</p>
<p>As it goes in life, our model wins some and loses some. How do we make it
actually learn from its experiences? How does the magic happen?</p>
<p>The REINFORCE algorithm is one of the basic workhorses of reinforcement learning, and
we&rsquo;ll start from there.
It&rsquo;s an example of a policy gradient method, which essentially means that it applies
the gradient descent techniques we know and love from basic ML to improve a policy network.
The good news is that it&rsquo;s fundamentally a simple method.</p>
<h2 id="the-basic-idea">The basic idea<a hidden class="anchor" aria-hidden="true" href="#the-basic-idea">#</a></h2>
<p>We want to reward the model for good moves and punish it for bad
ones, but the issue is that at the moment we make a move, it&rsquo;s usually not
obvious yet if it was good or bad. So for now let&rsquo;s just say that moves that ultimately
led to a win were probably good, and moves that led to a loss were probably bad.</p>
<p>If the model found itself in board state \(s\) and took an action \(a\), and it
turned out to be a winning move, we want the model to do more of that. The standard
deep learning way of achieving that is computing the gradient \(\nabla p_a(s)\) (with respect to the model weights)
of the probability \(p_a(s)\) of choosing \(a\), and then taking a gradient step of the
model weights in that direction; this naturally pushes \(p_a(s)\) higher.</p>
<p>How big should our step be? We could come up with all kinds of heuristics for that,
but why not simply stick to a formula that we already know works well?
If the model currently outputs probabilities \( p_i(s) \) for the seven possible actions
\( i = 0, \ldots, 6 \) in state \(s\) and the one we want it to actually take is \(a\),
that&rsquo;s nothing else but a standard classification problem.
The standard loss function for that is
<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">cross entropy loss</a>
with the simple formula</p>
\[
    \ell = -\log p_a(s).
\]<p>As an aside, if we compute the gradient of that, it&rsquo;s just</p>
\[
    \nabla\ell = - \frac{\nabla p_a(s)}{p_a(s)}
\]<p>and that kind of makes intuitive sense: if the probability \(p_a(s)\) we want to be high
is already close to 1, we take a smaller step, but if \(p_a(s)\) is small, the step is larger.</p>
<p>We have an additional bit of information, and that is the <strong>return</strong> we got from making
move \(a\); it&rsquo;s a real number \(G \in [-1,1] \) representing the cumulative reward for an
action, adjusted for future outcomes. We&rsquo;ll discuss it further below.</p>
<p>If we simply multiply our loss with the return, we get the complete REINFORCE
policy loss function for a single sample,</p>
\[
    \ell = -G \log p_a(s).
\]<p>It&rsquo;s clear that it takes larger steps for moves that gave us a higher return, and if
the move led to a loss and therefore \(G<0\), we actually take a gradient step in
the opposite direction, which is exactly what we want in order to discourage taking this
action in the future.</p>
<h2 id="assigning-returns">Assigning returns<a hidden class="anchor" aria-hidden="true" href="#assigning-returns">#</a></h2>
<p>If we play an entire game (often called a &ldquo;trajectory&rdquo; in RL) with \(n\) moves,
we end up with sequences</p>
\[
    (s_t), \quad (a_t), \qquad t=1,\ldots,n
\]<p>of board states and actions we took in those states.
To obtain the complete REINFORCE policy loss, we can simply sum up the single-state
loss function shown above. But we do need returns \(G_t \in [-1,1]\) for each of the
taken actions. How should we assign them?</p>
<p>We could simply set \(G_t = 1\) for all moves in won games and
\(G_t = -1\) for all moves in lost games, but that seems a bit overconfident.
Maybe we made a few suboptimal moves early on but still managed to pull off the win,
and we don&rsquo;t want to reward those early moves with the same weight.</p>
<p>On the other hand, if we set \(G_n=\pm1\) for the final winning or losing move only
and \(G_t=0\) for all previous moves, we don&rsquo;t learn anything at all about moves
that brought us into an ultimately advantageous position. The usual compromise
in REINFORCE is to choose a reward discount rate \(\gamma<1\) and compute the returns as</p>
\[
    G_t = R \gamma^{n-t}, \qquad t=1,\ldots,n
\]<p>where \(R\) is 1 for a win and -1 for a loss; common choices are \(\gamma\in[0.9,0.99]\).
This just means that the final move gets the full reward \(R\), and the previous moves
get weighted less and less the earlier in the game they occurred.
This is called <strong>discounted returns</strong>.</p>
<p>Of course, if we played an entire batch of games, we can simply assign the returns for
each game individually and then throw them all together into a full batch of
states, actions, and returns. Our final batched policy loss function is then simply</p>
\[
    \ell = - \sum_{t=1}^n G_t \log p_{a_t}(s_t).
\]<h2 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h2>
<p>Once we understood the formula, it&rsquo;s actually not too hard to implement in PyTorch.
In practice we don&rsquo;t take the gradient step directly, but pass it to an optimizer
like Adam, which usually performs better than standard SGD.
Here&rsquo;s a basic implementation:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_policy</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">states</span><span class="p">:</span>  <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>    <span class="c1"># (B, 6, 7) - all board states</span>
</span></span><span class="line"><span class="cl">    <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>    <span class="c1"># (B,)      - actions taken in those states</span>
</span></span><span class="line"><span class="cl">    <span class="n">returns</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>     <span class="c1"># (B,)      - discounted returns for the actions</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Update the policy model using the REINFORCE algorithm.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># compute model logit outputs for the batch</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>                                         <span class="c1"># (B, 7)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># mask out illegal moves and compute log probs of all actions</span>
</span></span><span class="line"><span class="cl">    <span class="n">masked_logits</span> <span class="o">=</span> <span class="n">mask_invalid_moves_batch</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>       <span class="c1"># (B, 7)</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">masked_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                <span class="c1"># (B, 7)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># select log probs of actually taken actions: (B, 7) -&gt; (B, 1) -&gt; (B,)</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_probs_taken</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">index</span><span class="o">=</span><span class="n">actions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># calculate REINFORCE loss = - sum(G_t * log p_(a_t)(s_t), t)</span>
</span></span><span class="line"><span class="cl">    <span class="n">policy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">returns</span> <span class="o">*</span> <span class="n">log_probs_taken</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># perform gradient update</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">policy_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span></code></pre></div><p>There&rsquo;s one new function here, <code>mask_invalid_moves_batch</code>, which performs the
masking operation for illegal moves described in the previous post for an entire
batch; simple stuff.</p>
<p>It&rsquo;s also worth pointing out the use of the
<a href="https://pytorch.org/docs/stable/generated/torch.gather.html"><code>torch.gather</code></a>
function: we use it to pick out from the tensor of all seven log-probabilities per sample
precisely the one corresponding to the action actually taken by indexing along the
second axis with the <code>actions</code> tensor. The tensor being used as an index needs to have
the same number of dimensions as the original tensor, which is why we need to call
<code>actions.unsqueeze(1)</code> to add a second singleton axis to <code>actions</code>.</p>
<p>With the REINFORCE algorithm implemented, we now have everything we need to start
training some models.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://c-f-h.github.io/tags/python/">Python</a></li>
      <li><a href="https://c-f-h.github.io/tags/pytorch/">PyTorch</a></li>
      <li><a href="https://c-f-h.github.io/tags/ml/">ML</a></li>
      <li><a href="https://c-f-h.github.io/tags/rl/">RL</a></li>
      <li><a href="https://c-f-h.github.io/tags/math/">Math</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://c-f-h.github.io/post/basic-setup-and-play/">
    <span class="title">« Prev</span>
    <br>
    <span>Basic Setup and Play</span>
  </a>
  <a class="next" href="https://c-f-h.github.io/post/policy-collapse/">
    <span class="title">Next »</span>
    <br>
    <span>A First Training Run and Policy Collapse</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://c-f-h.github.io/">cfh::blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
