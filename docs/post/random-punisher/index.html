<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Introducing a Benchmark Opponent | cfh::blog</title>
<meta name="keywords" content="Python, PyTorch, ML, RL">
<meta name="description" content="Last time we saw how the entropy bonus
enables self-play training without running into policy collapse.
However, the model we trained was quite small and probably not capable of very strong play.
Before we dive into the details of an improved model architecture, it would be very helpful
to have a decent, fixed benchmark to gauge our progress.
A benchmark opponent
The only model with fixed performance we have right now is the RandomPlayer from the
basic setup post. Obviously, that&rsquo;s not a challenging
bar to clear. But it turns out that with some small tweaks, we can turn the fully random
player into a formidable opponent for our starter models.">
<meta name="author" content="cfh">
<link rel="canonical" href="https://c-f-h.github.io/post/random-punisher/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.eb4f5ef1642754ddbc9d4a04f993a593b712d15d7b60b7a6a6dcc86954ff7b35.css" integrity="sha256-609e8WQnVN28nUoE&#43;ZOlk7cS0V17YLemptzIaVT/ezU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://c-f-h.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://c-f-h.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://c-f-h.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://c-f-h.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://c-f-h.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://c-f-h.github.io/post/random-punisher/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><head>
  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
  
  
  
</head><meta property="og:url" content="https://c-f-h.github.io/post/random-punisher/">
  <meta property="og:site_name" content="cfh::blog">
  <meta property="og:title" content="Introducing a Benchmark Opponent">
  <meta property="og:description" content="Last time we saw how the entropy bonus enables self-play training without running into policy collapse. However, the model we trained was quite small and probably not capable of very strong play. Before we dive into the details of an improved model architecture, it would be very helpful to have a decent, fixed benchmark to gauge our progress.
A benchmark opponent The only model with fixed performance we have right now is the RandomPlayer from the basic setup post. Obviously, that’s not a challenging bar to clear. But it turns out that with some small tweaks, we can turn the fully random player into a formidable opponent for our starter models.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-04-26T09:50:00+02:00">
    <meta property="article:modified_time" content="2025-04-26T09:50:00+02:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="PyTorch">
    <meta property="article:tag" content="ML">
    <meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Introducing a Benchmark Opponent">
<meta name="twitter:description" content="Last time we saw how the entropy bonus
enables self-play training without running into policy collapse.
However, the model we trained was quite small and probably not capable of very strong play.
Before we dive into the details of an improved model architecture, it would be very helpful
to have a decent, fixed benchmark to gauge our progress.
A benchmark opponent
The only model with fixed performance we have right now is the RandomPlayer from the
basic setup post. Obviously, that&rsquo;s not a challenging
bar to clear. But it turns out that with some small tweaks, we can turn the fully random
player into a formidable opponent for our starter models.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://c-f-h.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Introducing a Benchmark Opponent",
      "item": "https://c-f-h.github.io/post/random-punisher/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Introducing a Benchmark Opponent",
  "name": "Introducing a Benchmark Opponent",
  "description": "Last time we saw how the entropy bonus enables self-play training without running into policy collapse. However, the model we trained was quite small and probably not capable of very strong play. Before we dive into the details of an improved model architecture, it would be very helpful to have a decent, fixed benchmark to gauge our progress.\nA benchmark opponent The only model with fixed performance we have right now is the RandomPlayer from the basic setup post. Obviously, that\u0026rsquo;s not a challenging bar to clear. But it turns out that with some small tweaks, we can turn the fully random player into a formidable opponent for our starter models.\n",
  "keywords": [
    "Python", "PyTorch", "ML", "RL"
  ],
  "articleBody": "Last time we saw how the entropy bonus enables self-play training without running into policy collapse. However, the model we trained was quite small and probably not capable of very strong play. Before we dive into the details of an improved model architecture, it would be very helpful to have a decent, fixed benchmark to gauge our progress.\nA benchmark opponent The only model with fixed performance we have right now is the RandomPlayer from the basic setup post. Obviously, that’s not a challenging bar to clear. But it turns out that with some small tweaks, we can turn the fully random player into a formidable opponent for our starter models.\nThe algorithm is very simple:\nIf there is a winning move on the board, play it. If not, and if the opponent has a winning move, block it. Otherwise, play a random move. We call this model the RandomPunisher since, although it doesn’t have any concept of strategy, it will ruthlessly punish tactical mistakes.\nHere’s an implementation of that idea in PyTorch:\n@torch.jit.script def find_best_move(board): \"\"\" Finds a winning move for the current player (represented as +1) in the given board state (R, C). Then checks for potential winning moves of the opponent and blocks them. Otherwise, moves randomly. Returns logits, shape (C,). \"\"\" cols = board.shape[-1] for B in (board, -board): for c in range(cols): # Check if the move is valid if B[0, c] == 0: _, win = make_move_and_check(B, c) if win: choice = torch.tensor(c, device=board.device) logits = nn.functional.one_hot(choice, num_classes=cols).float() # Avoid log(0) by adding a small epsilon return torch.log(logits + 1e-12) return torch.zeros((cols,), dtype=torch.float32, device=board.device) class RandomPunisher(nn.Module): \"\"\"Plays a winning or blocking move if available, otherwise plays a random move.\"\"\" def forward(self, x): # Store original shape and determine batch size original_shape = x.shape if x.ndim == 2: # Single board state (R, C) x = x.unsqueeze(0) # Add a batch dimension: (1, R, C) batch_size = x.size(0) logits = torch.stack([find_best_move(x[i]) for i in range(batch_size)]) if len(original_shape) == 2: logits = logits.squeeze(0) # -\u003e (C,) return logits The function find_best_move is the core of the strategy; it calls the function make_move_and_check, which we already used in our earlier post, to check each valid move to see if it would result in a win. It does this first for the actual input board, and if no winning move was found, repeats the procedure for -board. This is the board from the opponent’s view and finds any potentially winning moves we have to block.\nFor any move it chooses, it applies log to a one-hot encoding (plus some small number) of that move so that we end up with a logit of 0 for the chosen move and sufficiently large negative logits for the others. After softmax, this effectively results in probability 1 for the chosen move.\nThe actual RandomPunisher module then simply calls this function in a loop for each board state in the input batch. This is not the most efficient implementation, but the use of the @torch.jit.script decorator, which tells PyTorch to just-in-time compile the function, goes a long way towards speeding up the generally slow Python loops.\nDespite the simplicity of this algorithm, it’s challenging for a model which doesn’t yet have a strong grasp of basic tactics to achieve a consistently positive win rate against this guy.\nFor instance, I continued the self-play loop for the SimpleMLPModel from the last post, and despite some attempts at tuning the hyperparameters (starting with a learning rate of 1e-3, then reducing to 1e-4; starting with an entropy bonus of 0.05, then decreasing to 0.03), it was difficult to get consistently beyond a 35% win rate against the RandomPunisher.\nPresumably, the simple MLP model lacks the capacity for strong play, and we need a better design, which is the topic of the next post.\n",
  "wordCount" : "638",
  "inLanguage": "en",
  "datePublished": "2025-04-26T09:50:00+02:00",
  "dateModified": "2025-04-26T09:50:00+02:00",
  "author":{
    "@type": "Person",
    "name": "cfh"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://c-f-h.github.io/post/random-punisher/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "cfh::blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://c-f-h.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://c-f-h.github.io/" accesskey="h" title="cfh::blog (Alt + H)">cfh::blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://c-f-h.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://c-f-h.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://c-f-h.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://c-f-h.github.io/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Introducing a Benchmark Opponent
    </h1>

  
    <div class="series-info">
      <div class="series-name">
        Series: <a href="/categories/connect-zero" onclick="document.getElementById('timeline-card').classList.toggle('collapsed'); return false;">Connect-Zero</a>
        (Part 8 of 11)
      </div>
      <nav class="series-navigation"><a href="/post/entropy-regularization/" title="Previous: Entropy Regularization">
            <span class="arrow">←</span> Previous
          </a><a href="/post/model-design/" title="Next: Model Design for Connect 4">
            Next <span class="arrow">→</span>
          </a></nav>
    </div>
    <div class="timeline-card collapsed" id="timeline-card">
    <div class="timeline">
      <div class="item">
          <div class="item-title">
            1.&nbsp;<a href="/post/connect-4/">Connect 4</a></div>
          <div class="item-date">April 20, 2025</div>
        </div><div class="item">
          <div class="item-title">
            2.&nbsp;<a href="/post/connect-zero/">Connect-Zero: Reinforcement Learning from Scratch</a></div>
          <div class="item-date">April 20, 2025</div>
        </div><div class="item">
          <div class="item-title">
            3.&nbsp;<a href="/post/basic-setup-and-play/">Basic Setup and Play</a></div>
          <div class="item-date">April 20, 2025</div>
        </div><div class="item">
          <div class="item-title">
            4.&nbsp;<a href="/post/the-reinforce-algorithm/">The REINFORCE Algorithm</a></div>
          <div class="item-date">April 20, 2025</div>
        </div><div class="item">
          <div class="item-title">
            5.&nbsp;<a href="/post/policy-collapse/">A First Training Run and Policy Collapse</a></div>
          <div class="item-date">April 21, 2025</div>
        </div><div class="item">
          <div class="item-title">
            6.&nbsp;<a href="/post/on-entropy/">On Entropy</a></div>
          <div class="item-date">April 23, 2025</div>
        </div><div class="item">
          <div class="item-title">
            7.&nbsp;<a href="/post/entropy-regularization/">Entropy Regularization</a></div>
          <div class="item-date">April 24, 2025</div>
        </div><div class="item current">
          <div class="item-title">
            8.&nbsp;Introducing a Benchmark Opponent</div>
          <div class="item-date">April 26, 2025 • You are here</div>
        </div><div class="item inactive">
          <div class="item-title">
            9.&nbsp;<a href="/post/model-design/">Model Design for Connect 4</a></div>
          <div class="item-date">April 28, 2025</div>
        </div><div class="item inactive">
          <div class="item-title">
            10.&nbsp;<a href="/post/reinforce-with-baseline/">REINFORCE with Baseline</a></div>
          <div class="item-date">April 29, 2025</div>
        </div><div class="item inactive">
          <div class="item-title">
            11.&nbsp;<a href="/post/implementing-rwb/">Implementing and Evaluating REINFORCE with Baseline</a></div>
          <div class="item-date">May 1, 2025</div>
        </div>
    </div>
  </div>
  


    <div class="post-meta"><span title='2025-04-26 09:50:00 +0200 CEST'>April 26, 2025</span>&nbsp;·&nbsp;cfh

</div>
  </header> 
  <div class="post-content"><p><a href="/post/entropy-regularization/">Last time</a> we saw how the entropy bonus
enables self-play training without running into policy collapse.
However, the model we trained was quite small and probably not capable of very strong play.
Before we dive into the details of an improved model architecture, it would be very helpful
to have a decent, fixed benchmark to gauge our progress.</p>
<h2 id="a-benchmark-opponent">A benchmark opponent<a hidden class="anchor" aria-hidden="true" href="#a-benchmark-opponent">#</a></h2>
<p>The only model with fixed performance we have right now is the <code>RandomPlayer</code> from the
<a href="/post/basic-setup-and-play/">basic setup post</a>. Obviously, that&rsquo;s not a challenging
bar to clear. But it turns out that with some small tweaks, we can turn the fully random
player into a formidable opponent for our starter models.</p>
<p>The algorithm is very simple:</p>
<ol>
<li>If there is a winning move on the board, play it.</li>
<li>If not, and if the opponent has a winning move, block it.</li>
<li>Otherwise, play a random move.</li>
</ol>
<p>We call this model the <code>RandomPunisher</code> since, although it doesn&rsquo;t have any concept of
strategy, it will ruthlessly punish tactical mistakes.</p>
<p>Here&rsquo;s an implementation of that idea in PyTorch:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="nd">@torch.jit.script</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">find_best_move</span><span class="p">(</span><span class="n">board</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Finds a winning move for the current player (represented as +1) in the
</span></span></span><span class="line"><span class="cl"><span class="s2">    given board state (R, C).
</span></span></span><span class="line"><span class="cl"><span class="s2">    Then checks for potential winning moves of the opponent and blocks them.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Otherwise, moves randomly.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns logits, shape (C,).
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cols</span> <span class="o">=</span> <span class="n">board</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">B</span> <span class="ow">in</span> <span class="p">(</span><span class="n">board</span><span class="p">,</span> <span class="o">-</span><span class="n">board</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cols</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Check if the move is valid</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">_</span><span class="p">,</span> <span class="n">win</span> <span class="o">=</span> <span class="n">make_move_and_check</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">win</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">choice</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">board</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">logits</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">choice</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">cols</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># Avoid log(0) by adding a small epsilon</span>
</span></span><span class="line"><span class="cl">                    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">logits</span> <span class="o">+</span> <span class="mf">1e-12</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">cols</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">board</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RandomPunisher</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Plays a winning or blocking move if available, otherwise plays a random move.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Store original shape and determine batch size</span>
</span></span><span class="line"><span class="cl">        <span class="n">original_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>        <span class="c1"># Single board state (R, C)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># Add a batch dimension: (1, R, C)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">find_best_move</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># -&gt; (C,)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">logits</span>
</span></span></code></pre></div><p>The function <code>find_best_move</code> is the core of the strategy; it calls the function
<code>make_move_and_check</code>, which we already used in our earlier post, to check each valid
move to see if it would result in a win. It does this first for the actual input
<code>board</code>, and if no winning move was found, repeats the procedure for <code>-board</code>. This
is the board from the opponent&rsquo;s view and finds any potentially winning moves we have to
block.</p>
<p>For any move it chooses, it applies log to a one-hot encoding (plus some small
number) of that move so that we end up with a logit of 0 for the chosen move and
sufficiently large negative logits for the others. After softmax, this effectively
results in probability 1 for the chosen move.</p>
<p>The actual <code>RandomPunisher</code> module then simply calls this function in a loop for each
board state in the input batch. This is not the most efficient implementation,
but the use of the <code>@torch.jit.script</code> decorator, which tells PyTorch to just-in-time
compile the function, goes a long way towards speeding up the generally slow Python loops.</p>
<p>Despite the simplicity of this algorithm, it&rsquo;s challenging for a model which
doesn&rsquo;t yet have a strong grasp of basic tactics to achieve
a consistently positive win rate against this guy.</p>
<p>For instance, I continued the self-play loop for the <code>SimpleMLPModel</code>
from <a href="/post/entropy-regularization/">the last post</a>,
and despite some attempts at tuning the hyperparameters
(starting with a learning rate of 1e-3, then reducing to 1e-4;
starting with an entropy bonus of 0.05, then decreasing to 0.03), it was difficult
to get consistently beyond a 35% win rate against the <code>RandomPunisher</code>.</p>
<p>Presumably, the simple MLP model lacks the capacity for strong play,
and we need a better design, which is the topic of <a href="/post/model-design/">the next post</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://c-f-h.github.io/tags/python/">Python</a></li>
      <li><a href="https://c-f-h.github.io/tags/pytorch/">PyTorch</a></li>
      <li><a href="https://c-f-h.github.io/tags/ml/">ML</a></li>
      <li><a href="https://c-f-h.github.io/tags/rl/">RL</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://c-f-h.github.io/post/entropy-regularization/">
    <span class="title">« Prev</span>
    <br>
    <span>Entropy Regularization</span>
  </a>
  <a class="next" href="https://c-f-h.github.io/post/model-design/">
    <span class="title">Next »</span>
    <br>
    <span>Model Design for Connect 4</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://c-f-h.github.io/">cfh::blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
