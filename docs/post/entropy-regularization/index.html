<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Entropy Regularization | cfh::blog</title>
<meta name="keywords" content="Python, PyTorch, ML, RL">
<meta name="description" content="Based on our discussion on entropy, our plan is to implement
entropy regularization via an entropy bonus in our loss function.

  
  
    
      
        ❕
        
          Example Code
        
      
      Runnable example code for this post:
connect-zero/train/example2-entropy.py.
    
  Implementing the entropy bonus
The formula for entropy which we have to implement,
\[
    H(p) = -\sum_{i=1}^{C} p_i \log p_i,
\]is simple enough: multiply the probabilities for the seven possible moves with their
log-probabilities, sum and negate. However, there is one numerical problem we
have to worry about: masking out an illegal move \(i\) leads to a zero probability
\(p_i=0\) and a log-probability \(\log p_i = -\infty\). However, due to the rules of
IEEE 754 floating point numbers, multiplying zero with \(\pm\infty\) is undefined and
therefore results in NaN (not a number). For the entropy formula, however, the
contribution should be 0.">
<meta name="author" content="cfh">
<link rel="canonical" href="https://c-f-h.github.io/post/entropy-regularization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.024295b3c968fbd469a11050839fd375a96747c3a5cff215e7f577090fe610f8.css" integrity="sha256-AkKVs8lo&#43;9RpoRBQg5/TdalnR8Olz/IV5/V3CQ/mEPg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://c-f-h.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://c-f-h.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://c-f-h.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://c-f-h.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://c-f-h.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://c-f-h.github.io/post/entropy-regularization/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><head>
  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
  
  
  <link rel="stylesheet" href="/connect4/connect4-game.css">
  <script src="/connect4/connect4-game.js"></script>
  
  
</head><meta property="og:url" content="https://c-f-h.github.io/post/entropy-regularization/">
  <meta property="og:site_name" content="cfh::blog">
  <meta property="og:title" content="Entropy Regularization">
  <meta property="og:description" content="Based on our discussion on entropy, our plan is to implement entropy regularization via an entropy bonus in our loss function.
❕ Example Code Runnable example code for this post:
connect-zero/train/example2-entropy.py.
Implementing the entropy bonus The formula for entropy which we have to implement,
\[H(p) = -\sum_{i=1}^{C} p_i \log p_i,\]is simple enough: multiply the probabilities for the seven possible moves with their log-probabilities, sum and negate. However, there is one numerical problem we have to worry about: masking out an illegal move \(i\) leads to a zero probability \(p_i=0\) and a log-probability \(\log p_i = -\infty\). However, due to the rules of IEEE 754 floating point numbers, multiplying zero with \(\pm\infty\) is undefined and therefore results in NaN (not a number). For the entropy formula, however, the contribution should be 0.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-04-24T20:15:00+02:00">
    <meta property="article:modified_time" content="2025-04-24T20:15:00+02:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="PyTorch">
    <meta property="article:tag" content="ML">
    <meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Entropy Regularization">
<meta name="twitter:description" content="Based on our discussion on entropy, our plan is to implement
entropy regularization via an entropy bonus in our loss function.

  
  
    
      
        ❕
        
          Example Code
        
      
      Runnable example code for this post:
connect-zero/train/example2-entropy.py.
    
  Implementing the entropy bonus
The formula for entropy which we have to implement,
\[
    H(p) = -\sum_{i=1}^{C} p_i \log p_i,
\]is simple enough: multiply the probabilities for the seven possible moves with their
log-probabilities, sum and negate. However, there is one numerical problem we
have to worry about: masking out an illegal move \(i\) leads to a zero probability
\(p_i=0\) and a log-probability \(\log p_i = -\infty\). However, due to the rules of
IEEE 754 floating point numbers, multiplying zero with \(\pm\infty\) is undefined and
therefore results in NaN (not a number). For the entropy formula, however, the
contribution should be 0.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://c-f-h.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Entropy Regularization",
      "item": "https://c-f-h.github.io/post/entropy-regularization/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Entropy Regularization",
  "name": "Entropy Regularization",
  "description": "Based on our discussion on entropy, our plan is to implement entropy regularization via an entropy bonus in our loss function.\n❕ Example Code Runnable example code for this post:\nconnect-zero/train/example2-entropy.py.\nImplementing the entropy bonus The formula for entropy which we have to implement,\n\\[\rH(p) = -\\sum_{i=1}^{C} p_i \\log p_i,\r\\]is simple enough: multiply the probabilities for the seven possible moves with their log-probabilities, sum and negate. However, there is one numerical problem we have to worry about: masking out an illegal move \\(i\\) leads to a zero probability \\(p_i=0\\) and a log-probability \\(\\log p_i = -\\infty\\). However, due to the rules of IEEE 754 floating point numbers, multiplying zero with \\(\\pm\\infty\\) is undefined and therefore results in NaN (not a number). For the entropy formula, however, the contribution should be 0.\n",
  "keywords": [
    "Python", "PyTorch", "ML", "RL"
  ],
  "articleBody": "Based on our discussion on entropy, our plan is to implement entropy regularization via an entropy bonus in our loss function.\n❕ Example Code Runnable example code for this post:\nconnect-zero/train/example2-entropy.py.\nImplementing the entropy bonus The formula for entropy which we have to implement,\n\\[\rH(p) = -\\sum_{i=1}^{C} p_i \\log p_i,\r\\]is simple enough: multiply the probabilities for the seven possible moves with their log-probabilities, sum and negate. However, there is one numerical problem we have to worry about: masking out an illegal move \\(i\\) leads to a zero probability \\(p_i=0\\) and a log-probability \\(\\log p_i = -\\infty\\). However, due to the rules of IEEE 754 floating point numbers, multiplying zero with \\(\\pm\\infty\\) is undefined and therefore results in NaN (not a number). For the entropy formula, however, the contribution should be 0.\nWe could explicitly check for NaNs, but a simple and very common workaround is to set the logits (i.e., the pre-softmax model outputs) for illegal moves not to \\(-\\infty\\), but instead add a large negative number, say -1e9, to them. This produces tiny probabilities which won’t disturb our result in any meaningful way; the resulting entropy contributions are essentially zero.\nIn practice, this means that the function masking invalid moves could look like this:\ndef mask_invalid_moves_batch(boards, logits, mask_value=-torch.inf): # check which top rows are already filled, and mask out those logits illegal_mask = torch.where(boards[:, 0, :] == 0, 0.0, mask_value) return logits + illegal_mask The tensor illegal_mask is 0 for valid moves and mask_value for invalid ones. To actually implement the entropy bonus, we modify the update_policy function from the REINFORCE algorithm as follows:\n# mask out illegal moves and compute logprobs of the actually taken actions # use finite mask instead of -inf to avoid nans in entropy masked_logits = mask_invalid_moves_batch(states, policy_logits_batch, mask_value=-1e9) log_probs = F.log_softmax(masked_logits, dim=1) # (B, C) entropy = -(log_probs * torch.exp(log_probs)).sum(1) # (B,) log_probs_taken = torch.gather(log_probs, dim=1, index=actions.unsqueeze(1)).squeeze() Note that since we already have the log-probs, we just apply exp to them to get the actual probabilities. Finally the loss is modified as follows:\npolicy_loss = -(returns * log_probs_taken).sum() # as before total_loss = policy_loss - ENTROPY_BONUS * entropy.sum() # add entropy term Then we call total_loss.backward() for backpropagation, with the remaining code unchanged.\nHere ENTROPY_BONUS is a hyperparameter which we set to 0.05 for now. You’ll need to tune this by monitoring your actual entropy plot. In our case, the choice 0.05 results in high but still reasonable entropy for early exploration of around 1.0. (Recall that this is roughly half the maximum possible entropy.)\nA new training run With these changes in place, we can start another self-play training run. We keep the model architecture and the hyperparameters mostly the same as in the first one, except that we increase the interval between model checkpoints from 100 to 400 batches to get a better look at the dynamics within these cycles.\nPlots of win rate, entropy, policy loss and game length over a self-play training run with entropy bonus 0.05.\nAgain we see the cyclical drops back towards 50% win rate which happen whenever we reach a new checkpoint, where the model initially starts playing itself. It then typically starts dominating its previous checkpoint rapidly, which is a great sign. Most importantly, this time there are no signs of collapse; game length varies considerably, and entropy generally seems to oscillate in the vicinity of 1.0 instead of collapsing to zero thanks to the entropy bonus \\(\\beta=0.05\\).\nGame analysis Here is a game the model, which has been trained only for a few minutes, played against itself. The first thing we notice is that it’s much longer at 26 moves.\nIt’s quite instructive to look at some moves together with the probabilities they were sampled with to get a feeling for how well the model understands various positions. I use the notation “Y3” for Yellow (the second player) playing in column 3. Percentages always refer to the chance \\(p_a(s)\\) of playing that move according to the softmax output.\nMove 8 (Y3): the top-rated move at 49% (entropy 1.36). The model already seems to favor moves which set up several connections (horizontal and vertical at once). Move 10 (Y4): the block is returned. Continuing its own column 3 was rated about even with both moves at around 34%. Move 13 (R2): a very highly rated move at 86% (entropy 0.58). Getting three in a row is very attractive to the model. Move 14 (Y2): and Yellow does the same with a 72% chance, even putting itself one move away from a win! Move 15 (R2): a blunder! Red should have blocked column 5. For some reason this pointless-looking move has 88% confidence. Move 16 (Y3): Yellow blunders back. In its defense, the winning move (5) actually had a 72% chance, but bad luck strikes and favors this 9% move over it. Move 17 (R5): this time Red succeeds in the block, although it’s still only 31% sure that this is the right thing to do. Move 24 (Y5): setting up another possible win with 60% confidence. Red misses it, and finally in move 26, Yellow brings it home with 37% confidence. This isn’t stellar play by any means. The model does seem to favor putting several of its pieces in a row, which is a good sign. On the other hand, it’s often completely blind to immediate one-move threats from its opponent, like in move 15. Then again, immediate wins seem to be rated relatively highly already, and missing the one in move 16 was just due to poor luck.\nOverall, defense seems to be much harder to grasp than offense, which perhaps isn’t surprising: a winning move ends the game immediately, but a losing blunder still requires the (currently poor) opponent to capitalize on it.\nBut what this shows is that entropy regularization is working: the model plays a wide variety of moves, explores different board states and slowly learns about the game.\nIt would absolutely be possible to continue training this model and see if it can reach a decent level of play, but perhaps it’s time we first invested a bit more effort into model design. In the next post, we first design a benchmark opponent to measure our progress in absolute terms.\n",
  "wordCount" : "1044",
  "inLanguage": "en",
  "datePublished": "2025-04-24T20:15:00+02:00",
  "dateModified": "2025-04-24T20:15:00+02:00",
  "author":{
    "@type": "Person",
    "name": "cfh"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://c-f-h.github.io/post/entropy-regularization/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "cfh::blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://c-f-h.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://c-f-h.github.io/" accesskey="h" title="cfh::blog (Alt + H)">cfh::blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://c-f-h.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://c-f-h.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://c-f-h.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://c-f-h.github.io/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Entropy Regularization
    </h1>
    <div class="post-meta"><span title='2025-04-24 20:15:00 +0200 CEST'>April 24, 2025</span>&nbsp;·&nbsp;cfh

</div>
  </header> 
  <div class="post-content"><p>Based on our <a href="/post/on-entropy/">discussion on entropy</a>, our plan is to implement
entropy regularization via an entropy bonus in our loss function.</p>

  
  
    <blockquote class="alert alert-important">
      <p class="alert-heading">
        ❕
        
          <b>Example Code</b>
        
      </p>
      <p>Runnable example code for this post:<br>
<a href="https://github.com/c-f-h/connect-zero/blob/main/train/example2-entropy.py"><code>connect-zero/train/example2-entropy.py</code></a>.</p>
    </blockquote>
  <h2 id="implementing-the-entropy-bonus">Implementing the entropy bonus<a hidden class="anchor" aria-hidden="true" href="#implementing-the-entropy-bonus">#</a></h2>
<p>The formula for entropy which we have to implement,</p>
\[
    H(p) = -\sum_{i=1}^{C} p_i \log p_i,
\]<p>is simple enough: multiply the probabilities for the seven possible moves with their
log-probabilities, sum and negate. However, there is one numerical problem we
have to worry about: masking out an illegal move \(i\) leads to a zero probability
\(p_i=0\) and a log-probability \(\log p_i = -\infty\). However, due to the rules of
IEEE 754 floating point numbers, multiplying zero with \(\pm\infty\) is undefined and
therefore results in NaN (not a number). For the entropy formula, however, the
contribution should be 0.</p>
<p>We could explicitly check for NaNs, but a simple
and very common workaround is to set the logits (i.e., the pre-softmax model outputs)
for illegal moves not to \(-\infty\), but instead
add a large negative number, say <code>-1e9</code>, to them. This produces tiny probabilities which
won&rsquo;t disturb our result in any meaningful way; the resulting entropy contributions
are essentially zero.</p>
<p>In practice, this means that the function masking invalid moves could look like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">mask_invalid_moves_batch</span><span class="p">(</span><span class="n">boards</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">mask_value</span><span class="o">=-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># check which top rows are already filled, and mask out those logits</span>
</span></span><span class="line"><span class="cl">    <span class="n">illegal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">boards</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">mask_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">illegal_mask</span>
</span></span></code></pre></div><p>The tensor <code>illegal_mask</code> is 0 for valid moves and <code>mask_value</code> for invalid ones.
To actually implement the entropy bonus, we modify the <code>update_policy</code> function from
the <a href="/post/the-reinforce-algorithm/">REINFORCE algorithm</a> as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl">    <span class="c1"># mask out illegal moves and compute logprobs of the actually taken actions</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># use finite mask instead of -inf to avoid nans in entropy</span>
</span></span><span class="line"><span class="cl">    <span class="n">masked_logits</span> <span class="o">=</span> <span class="n">mask_invalid_moves_batch</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">policy_logits_batch</span><span class="p">,</span> <span class="n">mask_value</span><span class="o">=-</span><span class="mf">1e9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">masked_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                <span class="c1"># (B, C)</span>
</span></span><span class="line"><span class="cl">    <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>           <span class="c1"># (B,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">log_probs_taken</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">actions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span></span></code></pre></div><p>Note that since we already have the log-probs, we just apply <code>exp</code> to them to get
the actual probabilities. Finally the loss is modified as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl">    <span class="n">policy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">returns</span> <span class="o">*</span> <span class="n">log_probs_taken</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>          <span class="c1"># as before</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">policy_loss</span> <span class="o">-</span> <span class="n">ENTROPY_BONUS</span> <span class="o">*</span> <span class="n">entropy</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># add entropy term</span>
</span></span></code></pre></div><p>Then we call <code>total_loss.backward()</code> for backpropagation, with the remaining code
unchanged.</p>
<p>Here <code>ENTROPY_BONUS</code> is a hyperparameter which we set to 0.05 for now.
You&rsquo;ll need to tune this by monitoring your actual entropy plot.
In our case, the choice 0.05 results in high but still reasonable entropy for early
exploration of around 1.0. (Recall that this is roughly half the maximum possible entropy.)</p>
<h2 id="a-new-training-run">A new training run<a hidden class="anchor" aria-hidden="true" href="#a-new-training-run">#</a></h2>
<p>With these changes in place, we can start another self-play training run.
We keep the model architecture and the hyperparameters mostly the same as in
<a href="/post/policy-collapse/">the first one</a>,
except that we increase the interval between model checkpoints from 100 to 400 batches
to get a better look at the dynamics within these cycles.</p>
<figure class="align-center ">
    <img loading="lazy" src="no_collapse2.png#center"
         alt="Training plots without policy collapse"/> <figcaption>
            <p>Plots of win rate, entropy, policy loss and game length over a self-play training run with entropy bonus 0.05.</p>
        </figcaption>
</figure>

<p>Again we see the cyclical drops back towards 50% win rate which happen whenever we
reach a new checkpoint, where the model initially starts playing itself.
It then typically starts dominating its previous checkpoint rapidly, which is
a great sign.
Most importantly, this time
there are no signs of collapse; game length varies considerably, and entropy generally
seems to oscillate in the vicinity of 1.0 instead of collapsing to zero thanks to
the entropy bonus \(\beta=0.05\).</p>
<h2 id="game-analysis">Game analysis<a hidden class="anchor" aria-hidden="true" href="#game-analysis">#</a></h2>
<p>Here is a game the model, which has been trained only for a few minutes, played against
itself. The first thing we notice is that it&rsquo;s much longer at 26 moves.</p>
<div id="game-container" class="connect4-container"
    data-human="-1" data-cpu="-1"
    data-movelist="[5, 4, 1, 1, 3, 2, 1, 2, 3, 3, 2, 2, 1, 1, 1, 2, 4, 4, 4, 2, 3, 5, 0, 4, 0, 3]">
</div>
<p>It&rsquo;s quite instructive to look at some moves together with the probabilities they were
sampled with to get a feeling for how well the model understands various positions.
I use the notation &ldquo;Y3&rdquo; for Yellow (the second player) playing in column 3.
Percentages always refer to the chance \(p_a(s)\) of playing that move according to the
softmax output.</p>
<ul>
<li>Move 8 (Y3): the top-rated move at 49% (entropy 1.36). The model already seems to favor
moves which set up several connections (horizontal and vertical at once).</li>
<li>Move 10 (Y4): the block is returned. Continuing its own column 3 was rated about even with
both moves at around 34%.</li>
<li>Move 13 (R2): a very highly rated move at 86% (entropy 0.58). Getting three in a row is
very attractive to the model.</li>
<li>Move 14 (Y2): and Yellow does the same with a 72% chance, even putting itself one move
away from a win!</li>
<li>Move 15 (R2): a blunder! Red should have blocked column 5. For some reason this
pointless-looking move has 88% confidence.</li>
<li>Move 16 (Y3): Yellow blunders back. In its defense, the winning move (5) actually had a
72% chance, but bad luck strikes and favors this 9% move over it.</li>
<li>Move 17 (R5): this time Red succeeds in the block, although it&rsquo;s still only 31% sure that
this is the right thing to do.</li>
<li>Move 24 (Y5): setting up another possible win with 60% confidence. Red misses it, and
finally in move 26, Yellow brings it home with 37% confidence.</li>
</ul>
<p>This isn&rsquo;t stellar play by any means. The model does seem to favor putting several
of its pieces in a row, which is a good sign. On the other hand, it&rsquo;s often completely blind
to immediate one-move threats from its opponent, like in move 15. Then again, immediate
wins seem to be rated relatively highly already, and missing the one in move 16 was just
due to poor luck.</p>
<p>Overall, defense seems to be much harder to grasp than offense,
which perhaps isn&rsquo;t surprising: a winning move ends the game immediately, but
a losing blunder still requires the (currently poor) opponent to capitalize on it.</p>
<p>But what this shows is that entropy regularization is working: the model plays a wide
variety of moves, explores different board states and slowly learns about the game.</p>
<p>It would absolutely be possible to continue training this model and see if it can reach
a decent level of play, but perhaps it&rsquo;s time we first invested a bit more effort into
model design. In <a href="/post/random-punisher/">the next post</a>, we first design a
benchmark opponent to measure our progress in absolute terms.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://c-f-h.github.io/tags/python/">Python</a></li>
      <li><a href="https://c-f-h.github.io/tags/pytorch/">PyTorch</a></li>
      <li><a href="https://c-f-h.github.io/tags/ml/">ML</a></li>
      <li><a href="https://c-f-h.github.io/tags/rl/">RL</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://c-f-h.github.io/post/on-entropy/">
    <span class="title">« Prev</span>
    <br>
    <span>On Entropy</span>
  </a>
  <a class="next" href="https://c-f-h.github.io/post/random-punisher/">
    <span class="title">Next »</span>
    <br>
    <span>Introducing a Benchmark Opponent</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://c-f-h.github.io/">cfh::blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
