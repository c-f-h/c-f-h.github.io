<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Actor-Critic Algorithms | cfh::blog</title>
<meta name="keywords" content="ML, RL, Math">
<meta name="description" content="After implementing and evaluating REINFORCE with baseline,
we found that it can produce strong models, but takes a long time to learn an accurate
value function due to the high variance of the
Monte Carlo samples.
In this post, we&rsquo;ll look at Actor-Critic methods, and in particular the
Advantage Actor-Critic (A2C) algorithm1,
a synchronous version of the earlier Asynchronous Advantage Actor-Critic (A3C) method,
as a way to remedy this.
Before we start, recall that we introduced a value network as a component of our model; this remains the same for A2C, and in fact
we don&rsquo;t need to modify the network architecture at all to use this newer algorithm.
Our model still consists of a residual CNN backbone, a policy head and a value head.
The value head serves as the &ldquo;critic,&rdquo; whereas the policy head is the &ldquo;actor&rdquo;.">
<meta name="author" content="cfh">
<link rel="canonical" href="https://c-f-h.github.io/post/actor-critic/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5c33e5d3f9279b49f845e940304f13c1593579964c46446a0b14ec2d6bdd02c1.css" integrity="sha256-XDPl0/knm0n4RelAME8TwVk1eZZMRkRqCxTsLWvdAsE=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://c-f-h.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://c-f-h.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://c-f-h.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://c-f-h.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://c-f-h.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://c-f-h.github.io/post/actor-critic/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><head>
  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
  
  
  
</head><meta property="og:url" content="https://c-f-h.github.io/post/actor-critic/">
  <meta property="og:site_name" content="cfh::blog">
  <meta property="og:title" content="Actor-Critic Algorithms">
  <meta property="og:description" content="After implementing and evaluating REINFORCE with baseline, we found that it can produce strong models, but takes a long time to learn an accurate value function due to the high variance of the Monte Carlo samples. In this post, we’ll look at Actor-Critic methods, and in particular the Advantage Actor-Critic (A2C) algorithm1, a synchronous version of the earlier Asynchronous Advantage Actor-Critic (A3C) method, as a way to remedy this.
Before we start, recall that we introduced a value network as a component of our model; this remains the same for A2C, and in fact we don’t need to modify the network architecture at all to use this newer algorithm. Our model still consists of a residual CNN backbone, a policy head and a value head. The value head serves as the “critic,” whereas the policy head is the “actor”.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-05-08T21:56:00+02:00">
    <meta property="article:modified_time" content="2025-05-08T21:56:00+02:00">
    <meta property="article:tag" content="ML">
    <meta property="article:tag" content="RL">
    <meta property="article:tag" content="Math">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Actor-Critic Algorithms">
<meta name="twitter:description" content="After implementing and evaluating REINFORCE with baseline,
we found that it can produce strong models, but takes a long time to learn an accurate
value function due to the high variance of the
Monte Carlo samples.
In this post, we&rsquo;ll look at Actor-Critic methods, and in particular the
Advantage Actor-Critic (A2C) algorithm1,
a synchronous version of the earlier Asynchronous Advantage Actor-Critic (A3C) method,
as a way to remedy this.
Before we start, recall that we introduced a value network as a component of our model; this remains the same for A2C, and in fact
we don&rsquo;t need to modify the network architecture at all to use this newer algorithm.
Our model still consists of a residual CNN backbone, a policy head and a value head.
The value head serves as the &ldquo;critic,&rdquo; whereas the policy head is the &ldquo;actor&rdquo;.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://c-f-h.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Actor-Critic Algorithms",
      "item": "https://c-f-h.github.io/post/actor-critic/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Actor-Critic Algorithms",
  "name": "Actor-Critic Algorithms",
  "description": "After implementing and evaluating REINFORCE with baseline, we found that it can produce strong models, but takes a long time to learn an accurate value function due to the high variance of the Monte Carlo samples. In this post, we\u0026rsquo;ll look at Actor-Critic methods, and in particular the Advantage Actor-Critic (A2C) algorithm1, a synchronous version of the earlier Asynchronous Advantage Actor-Critic (A3C) method, as a way to remedy this.\nBefore we start, recall that we introduced a value network as a component of our model; this remains the same for A2C, and in fact we don\u0026rsquo;t need to modify the network architecture at all to use this newer algorithm. Our model still consists of a residual CNN backbone, a policy head and a value head. The value head serves as the \u0026ldquo;critic,\u0026rdquo; whereas the policy head is the \u0026ldquo;actor\u0026rdquo;.\n",
  "keywords": [
    "ML", "RL", "Math"
  ],
  "articleBody": "After implementing and evaluating REINFORCE with baseline, we found that it can produce strong models, but takes a long time to learn an accurate value function due to the high variance of the Monte Carlo samples. In this post, we’ll look at Actor-Critic methods, and in particular the Advantage Actor-Critic (A2C) algorithm1, a synchronous version of the earlier Asynchronous Advantage Actor-Critic (A3C) method, as a way to remedy this.\nBefore we start, recall that we introduced a value network as a component of our model; this remains the same for A2C, and in fact we don’t need to modify the network architecture at all to use this newer algorithm. Our model still consists of a residual CNN backbone, a policy head and a value head. The value head serves as the “critic,” whereas the policy head is the “actor”.\nBootstrapping value In our previous algorithms, we used the discounted return \\(G\\) for the move \\(a\\) we made in state \\(s\\) in order to encourage or discourage similar actions in the future. For a winning game, \\(G\\) slowly increased to a final value of \\(\\pm1\\) for a winning/losing game. This gave us a general idea of which moves led to wins and losses, but did not assign precise rewards to individual moves.\nSay the action \\(a\\) has left us in a new state \\(s'\\). Temporal Difference (TD) learning is the idea that we already have an estimate \\(v(s')\\) for the value of this next state (in our case, from our value head) and can use it to update our knowledge of \\(v(s)\\).\nSuppose that the move \\(a\\) gave us an immediate reward \\(R\\); in our case, \\(R\\) will usually be 0, except if the move won the game, in which case \\(R=1\\). We also again use a reward discount rate \\(\\gamma\\le1\\) which balances immediate against future rewards. Then one-step temporal differencing says that we should update our value function via\n\\[ v(s) \\gets R + \\gamma v(s'). \\]In other words, the target value for the current state is the immediate reward \\(R\\) plus the discounted value of the next state \\(v(s')\\). The symbol \\(\\gets\\) shouldn’t be taken too literally: it just means that we will use a gradient update (via a squared error loss function) to pull \\(v(s)\\) towards the value of the right-hand side.2 To achieve that we’ll replace the value loss function with\n\\[ \\ell^{\\textsf{TD}} = \\alpha \\left(R + \\gamma v(s') - v(s)\\right)^2, \\]where \\(\\alpha \u003e 0\\) is again a weighting hyperparameter. We have to read this carefully, though: the gradient should only be taken for the \\(v(s)\\) part; the target value \\(v(s')\\) should be treated as constant for the purpose of the gradient step.\nSo it’s a slightly weird idea because we use the value function \\(v(\\cdot)\\) to essentially construct itself. This is known as bootstrapping, much like Baron Munchausen pulling himself, horse and all, out of a swamp (although he pulled himself out by his pigtail rather than his bootstraps).\nIn practice, for our Connect 4 setting where rewards are only assigned at the end of a game, this means that the method learns “backwards in time”: it first has to learn the value of terminal game states and can then transport this information backwards to earlier states through the TD mechanism.\nThere is one edge case we have to treat specially, namely if \\(s'\\) is a terminal state because the game ended in a win or draw. Our value head never sees such states during training, but we can simply fix their values as \\(v(s') = 0\\) because no further rewards are possible in a terminal state.\nBias and variance In our earlier discussion, we concluded that Monte Carlo sampling (as used in REINFORCE) has high variance, but low bias. For bootstrapping, the situation is reversed: the estimate has low variance because it depends only on the value of the successive state, not the entire trajectory, but high bias because it relies on the current estimate of the value function which may still be far from accurate.\nIn terms of credit assignment, this approach has clear advantages: instead of ramping up smoothly over the duration of a game, the bootstrapped valuecan more sharply distinguish key decision points such as blunders or forced wins. If the next state \\(s'\\) leaves our opponent with a clear winning move, then this should be reflected in a \\(v(s')\\) close to -1 and therefore pull the valuation for \\(v(s)\\) towards -1 as well.\nAnother way to look at it is that this bootstrapping technique uses one step of lookahead to improve the value estimate. There are more advanced techniques which use several time steps or blend them with Monte Carlo estimates to achieve a tradeoff of variance and bias, but for now we stick to this simplest variant.\nUpdating the policy network The update of the policy network (or “actor” in the actor-critic nomenclature) works similarly to REINFORCE with baseline by computing an advantage \\(A\\) and using that to weight the log loss. However, we use a different formula for the advantage:\n\\[ A = R + \\gamma v(s') - v(s). \\]Note that this is just the error term of the value update and comes with the same tradeoffs: by replacing the observed discounted return \\(G\\) from the baseline version with the bootstrapped value estimate \\(R + \\gamma v(s')\\), it has higher bias due to a potentially inaccurate value, but the variance is lower, leading to further variance reduction compared to REINFORCE and its baseline variant.\nThe total loss function for A2C, including an entropy bonus to encourage exploration, is then\n\\[ \\ell^{\\textsf{A2C}} = \\underbrace{-A \\log p_a(s)}_{\\textsf{policy loss}} + \\underbrace{\\alpha (R + \\gamma v(s') - v(s))^2}_{\\textsf{value loss}} - \\underbrace{\\beta H(p(s))}_{\\textsf{entropy bonus}}. \\]To summarize, this version of A2C combines one-step bootstrapping from TD learning with a baseline-adjusted policy update. It combines the strengths of REINFORCE with baseline (via advantage estimation) and TD learning (lower-variance updates). In the next installment of the series, we’ll implement the A2C method for our Connect 4 agent.\nThese methods have quite a long history and have evolved over time, and it’s often hard to pin down exact references. Richard Sutton’s PhD thesis (1984) and the book by Sutton and Barto (1998), “Reinforcement Learning: An Introduction,” already describe actor-critic methods with TD learning. In the deep learning era, these ideas could then be applied to much more complex problems. The A3C (asynchronous advantage actor-critic) method was introduced by Mnih et al. (2016), “Asynchronous Methods for Deep Reinforcement Learning”. A2C was then popularized by OpenAI and others as a synchronous, simpler and often more efficient variant of A3C. ↩︎\nIn classical TD learning algorithms, these updates were actually applied to values in a table directly, modified only by a learning rate. ↩︎\n",
  "wordCount" : "1121",
  "inLanguage": "en",
  "datePublished": "2025-05-08T21:56:00+02:00",
  "dateModified": "2025-05-08T21:56:00+02:00",
  "author":{
    "@type": "Person",
    "name": "cfh"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://c-f-h.github.io/post/actor-critic/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "cfh::blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://c-f-h.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://c-f-h.github.io/" accesskey="h" title="cfh::blog (Alt + H)">cfh::blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://c-f-h.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://c-f-h.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://c-f-h.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://c-f-h.github.io/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Actor-Critic Algorithms
    </h1>

  
    <div class="series-info">
      <div class="series-name">
        Series: <a href="/categories/connect-zero" onclick="document.getElementById('timeline-card').classList.toggle('collapsed'); return false;">Connect-Zero</a>
        (Part 12 of 12)
      </div>
      <nav class="series-navigation"><a href="/post/implementing-rwb/" title="Previous: Implementing and Evaluating REINFORCE with Baseline">
            <span class="arrow">←</span> Previous
          </a></nav>
    </div>
    <div class="timeline-card collapsed" id="timeline-card">
    <div class="timeline">
      <div class="item">
          <div class="item-title">
            1.&nbsp;<a href="/post/connect-4/">Connect 4</a></div>
          <div class="item-date">April 20, 2025</div>
        </div><div class="item">
          <div class="item-title">
            2.&nbsp;<a href="/post/connect-zero/">Connect-Zero: Reinforcement Learning from Scratch</a></div>
          <div class="item-date">April 20, 2025</div>
        </div><div class="item">
          <div class="item-title">
            3.&nbsp;<a href="/post/basic-setup-and-play/">Basic Setup and Play</a></div>
          <div class="item-date">April 20, 2025</div>
        </div><div class="item">
          <div class="item-title">
            4.&nbsp;<a href="/post/the-reinforce-algorithm/">The REINFORCE Algorithm</a></div>
          <div class="item-date">April 20, 2025</div>
        </div><div class="item">
          <div class="item-title">
            5.&nbsp;<a href="/post/policy-collapse/">A First Training Run and Policy Collapse</a></div>
          <div class="item-date">April 21, 2025</div>
        </div><div class="item">
          <div class="item-title">
            6.&nbsp;<a href="/post/on-entropy/">On Entropy</a></div>
          <div class="item-date">April 23, 2025</div>
        </div><div class="item">
          <div class="item-title">
            7.&nbsp;<a href="/post/entropy-regularization/">Entropy Regularization</a></div>
          <div class="item-date">April 24, 2025</div>
        </div><div class="item">
          <div class="item-title">
            8.&nbsp;<a href="/post/random-punisher/">Introducing a Benchmark Opponent</a></div>
          <div class="item-date">April 26, 2025</div>
        </div><div class="item">
          <div class="item-title">
            9.&nbsp;<a href="/post/model-design/">Model Design for Connect 4</a></div>
          <div class="item-date">April 28, 2025</div>
        </div><div class="item">
          <div class="item-title">
            10.&nbsp;<a href="/post/reinforce-with-baseline/">REINFORCE with Baseline</a></div>
          <div class="item-date">April 29, 2025</div>
        </div><div class="item">
          <div class="item-title">
            11.&nbsp;<a href="/post/implementing-rwb/">Implementing and Evaluating REINFORCE with Baseline</a></div>
          <div class="item-date">May 1, 2025</div>
        </div><div class="item current">
          <div class="item-title">
            12.&nbsp;Actor-Critic Algorithms</div>
          <div class="item-date">May 8, 2025 • You are here</div>
        </div>
    </div>
  </div>
  


    <div class="post-meta"><span title='2025-05-08 21:56:00 +0200 CEST'>May 8, 2025</span>&nbsp;·&nbsp;cfh

</div>
  </header> 
  <div class="post-content"><p>After <a href="/post/implementing-rwb/">implementing and evaluating</a> REINFORCE with baseline,
we found that it can produce strong models, but takes a long time to learn an accurate
value function due to the high variance of the
<a href="/post/reinforce-with-baseline/#monte-carlo-sampling">Monte Carlo samples</a>.
In this post, we&rsquo;ll look at Actor-Critic methods, and in particular the
Advantage Actor-Critic (A2C) algorithm<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>,
a synchronous version of the earlier Asynchronous Advantage Actor-Critic (A3C) method,
as a way to remedy this.</p>
<p>Before we start, recall that we introduced a <a href="/post/reinforce-with-baseline/#the-value-network">value network</a> as a component of our model; this remains the same for A2C, and in fact
we don&rsquo;t need to modify the network architecture at all to use this newer algorithm.
Our model still consists of a residual CNN backbone, a policy head and a value head.
The value head serves as the &ldquo;critic,&rdquo; whereas the policy head is the &ldquo;actor&rdquo;.</p>
<h2 id="bootstrapping-value">Bootstrapping value<a hidden class="anchor" aria-hidden="true" href="#bootstrapping-value">#</a></h2>
<p>In our previous algorithms, we used the discounted return \(G\) for the move \(a\) we made in
state \(s\) in order to encourage or discourage similar actions in the future.
For a winning game, \(G\) slowly increased to a final value of \(\pm1\) for a winning/losing game.
This gave us a general idea of which moves led to wins and losses, but did not assign
precise rewards to individual moves.</p>
<p>Say the action \(a\) has left us in a new state \(s'\). <strong>Temporal Difference (TD) learning</strong>
is the idea that we already have an estimate \(v(s')\) for the value of this next state
(in our case, from our value head) and can use it to update our knowledge of \(v(s)\).</p>
<p>Suppose that the move \(a\) gave us an immediate reward \(R\); in our case, \(R\) will
usually be 0, except if the move won the game, in which case \(R=1\).
We also again use a reward discount rate \(\gamma\le1\) which balances immediate against
future rewards. Then one-step temporal differencing says that we should update
our value function via</p>
\[
    v(s) \gets R + \gamma v(s').
\]<p>In other words, the target value for the current state is the immediate reward \(R\) plus the
discounted value of the next state \(v(s')\).
The symbol \(\gets\) shouldn&rsquo;t be taken too literally:
it just means that we will use a gradient update (via a squared error loss function) to pull
\(v(s)\) towards the value of the right-hand side.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>
To achieve that we&rsquo;ll replace the value loss function with</p>
\[
    \ell^{\textsf{TD}} = \alpha \left(R + \gamma v(s') - v(s)\right)^2,
\]<p>where \(\alpha > 0\) is again a weighting hyperparameter. We have to read this carefully,
though: the gradient should only be taken for the \(v(s)\) part; the target value
\(v(s')\) should be treated as constant for the purpose of the gradient step.</p>
<p>So it&rsquo;s a slightly weird idea because we use the value function \(v(\cdot)\) to essentially
construct itself.
This is known as <strong><a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrapping</a></strong>,
much like Baron Munchausen <a href="https://en.wikipedia.org/wiki/Bootstrapping#/media/File:Zentralbibliothek_Solothurn_-_M%C3%BCnchhausen_zieht_sich_am_Zopf_aus_dem_Sumpf_-_a0400.tif">pulling himself, horse and all, out of a swamp</a>
(although he pulled himself out by his pigtail rather than his bootstraps).</p>
<p>In practice, for our Connect 4 setting where rewards are only assigned at the end of a game, this
means that the method learns &ldquo;backwards in time&rdquo;: it first has to learn the value of
terminal game states and can then transport this information backwards to earlier states through
the TD mechanism.</p>
<p>There is one edge case we have to treat specially, namely if \(s'\) is a terminal state because
the game ended in a win or draw. Our value head never sees such states during training, but
we can simply fix their values as \(v(s') = 0\) because no further rewards are
possible in a terminal state.</p>
<h2 id="bias-and-variance">Bias and variance<a hidden class="anchor" aria-hidden="true" href="#bias-and-variance">#</a></h2>
<p>In our <a href="/post/reinforce-with-baseline/#monte-carlo-sampling">earlier discussion</a>, we
concluded that Monte Carlo sampling (as used in REINFORCE) has high variance, but low bias.
For bootstrapping, the situation is reversed: the estimate has <strong>low variance</strong> because it
depends only on the value of the successive state, not the entire trajectory, but <strong>high bias</strong>
because it relies on the current estimate of the value function which may still be far from
accurate.</p>
<p>In terms of credit assignment, this approach has clear advantages: instead of ramping
up smoothly over the duration of a game, the bootstrapped valuecan more sharply distinguish key
decision points such as blunders or forced wins.
If the next state \(s'\) leaves our opponent with a clear winning
move, then this should be reflected in a \(v(s')\) close to -1 and therefore pull the valuation
for \(v(s)\) towards -1 as well.</p>
<p>Another way to look at it is that this bootstrapping technique uses one step of lookahead to
improve the value estimate. There are more advanced techniques which use several time steps
or blend them with Monte Carlo estimates to achieve a tradeoff of variance and bias,
but for now we stick to this simplest variant.</p>
<h2 id="updating-the-policy-network">Updating the policy network<a hidden class="anchor" aria-hidden="true" href="#updating-the-policy-network">#</a></h2>
<p>The update of the policy network (or &ldquo;actor&rdquo; in the actor-critic nomenclature) works
similarly to <a href="/post/reinforce-with-baseline/#using-value-to-estimate-advantage">REINFORCE with baseline</a> by computing an advantage \(A\) and using that to
weight the log loss. However, we use a different formula for the advantage:</p>
\[
    A = R + \gamma v(s') - v(s).
\]<p>Note that this is just the error term of the value update and comes with the same
tradeoffs: by replacing the observed discounted return \(G\) from the baseline version
with the bootstrapped value estimate \(R + \gamma v(s')\),
it has higher bias due to a potentially inaccurate value, but the variance is lower,
leading to further variance reduction compared to REINFORCE and its baseline variant.</p>
<p>The total loss function for A2C, including an
<a href="/post/entropy-regularization/">entropy bonus</a> to encourage exploration, is then</p>
\[
    \ell^{\textsf{A2C}} = \underbrace{-A \log p_a(s)}_{\textsf{policy loss}}
    + \underbrace{\alpha (R + \gamma v(s') - v(s))^2}_{\textsf{value loss}}
    - \underbrace{\beta H(p(s))}_{\textsf{entropy bonus}}.
\]<p>To summarize, this version of A2C combines one-step bootstrapping from TD learning with a
baseline-adjusted policy update.
It combines the strengths of REINFORCE with baseline (via advantage estimation) and TD learning
(lower-variance updates).
In the next installment of the series, we&rsquo;ll implement the A2C method for our Connect 4 agent.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>These methods have quite a long history and have evolved over time, and it&rsquo;s often hard to
pin down exact references.
Richard Sutton&rsquo;s PhD thesis (1984) and the book by
Sutton and Barto (1998), <em>&ldquo;Reinforcement Learning: An Introduction,&rdquo;</em> already describe
actor-critic methods with TD learning.
In the deep learning era, these ideas could then be applied to much more complex problems.
The A3C (asynchronous advantage actor-critic) method was introduced by
Mnih et al. (2016),
<a href="https://proceedings.mlr.press/v48/mniha16.html">&ldquo;Asynchronous Methods for Deep Reinforcement Learning&rdquo;</a>.
A2C was then <a href="https://openai.com/index/openai-baselines-acktr-a2c/">popularized by OpenAI</a> and others as a
synchronous, simpler and often more efficient variant of A3C.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>In classical <a href="https://en.wikipedia.org/wiki/Temporal_difference_learning">TD learning</a>
algorithms, these updates were actually applied to values in a table directly,
modified only by a learning rate.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://c-f-h.github.io/tags/ml/">ML</a></li>
      <li><a href="https://c-f-h.github.io/tags/rl/">RL</a></li>
      <li><a href="https://c-f-h.github.io/tags/math/">Math</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://c-f-h.github.io/post/implementing-rwb/">
    <span class="title">« Prev</span>
    <br>
    <span>Implementing and Evaluating REINFORCE with Baseline</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://c-f-h.github.io/">cfh::blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
