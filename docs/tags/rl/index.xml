<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>RL on cfh::blog</title>
    <link>https://c-f-h.github.io/tags/rl/</link>
    <description>Recent content in RL on cfh::blog</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 May 2025 23:05:00 +0200</lastBuildDate>
    <atom:link href="https://c-f-h.github.io/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Implementing and Evaluating REINFORCE with Baseline</title>
      <link>https://c-f-h.github.io/post/implementing-rwb/</link>
      <pubDate>Thu, 01 May 2025 23:05:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/implementing-rwb/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/reinforce-with-baseline/&#34;&gt;Having introduced&lt;/a&gt; REINFORCE with baseline on a
conceptual level, let&amp;rsquo;s implement it for our Connect 4-playing CNN model.&lt;/p&gt;
&lt;p&gt;Runnable example code for this post is at
&lt;a href=&#34;https://github.com/c-f-h/connect-zero/blob/main/train/example3-rwb.py&#34;&gt;&lt;code&gt;connect-zero/train/example3-rwb.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;adding-the-value-head&#34;&gt;Adding the value head&lt;/h2&gt;
&lt;p&gt;In the constructor of the &lt;a href=&#34;https://c-f-h.github.io/post/model-design/&#34;&gt;&lt;code&gt;Connect4CNN&lt;/code&gt; model class&lt;/a&gt;,
we set up the new network for estimating the board state value \(v(s)\)
which will consume the same 448 downsampled features that the policy head receives:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_head&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tanh&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It&amp;rsquo;s very similar in structure to the MLP policy head, with some minor differences:&lt;/p&gt;</description>
    </item>
    <item>
      <title>REINFORCE with Baseline</title>
      <link>https://c-f-h.github.io/post/reinforce-with-baseline/</link>
      <pubDate>Tue, 29 Apr 2025 08:42:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/reinforce-with-baseline/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/model-design/&#34;&gt;In the previous post&lt;/a&gt;, we introduced a stronger model
but observed that it&amp;rsquo;s quite challenging to achieve
a high level of play with &lt;a href=&#34;https://c-f-h.github.io/post/the-reinforce-algorithm/&#34;&gt;basic REINFORCE&lt;/a&gt;,
due to the high variance and noisy gradients of the algorithm which often lead to unstable
learning and slow convergence.
Our first step towards more advanced algorithms is a modification called
&amp;ldquo;REINFORCE with baseline&amp;rdquo; (see, e.g.,
&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&#34;&gt;Sutton et al. (2000)&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;the-value-network&#34;&gt;The value network&lt;/h2&gt;
&lt;p&gt;Given a board state \(s\), &lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;recall that&lt;/a&gt; our model
currently outputs seven raw logits which are then
transformed via softmax into the probability distribution \(p(s)\) over the seven possible
moves. Many advanced algorithms in RL assume that our network also outputs
a second piece of information: the &lt;strong&gt;value&lt;/strong&gt; \(v(s)\), a number between -1 and 1 which,
roughly speaking, gives an estimate of how confident the model is in winning from the
current position.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introducing a Benchmark Opponent</title>
      <link>https://c-f-h.github.io/post/random-punisher/</link>
      <pubDate>Sat, 26 Apr 2025 09:50:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/random-punisher/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/entropy-regularization/&#34;&gt;Last time&lt;/a&gt; we saw how the entropy bonus
enables self-play training without running into policy collapse.
However, the model we trained was quite small and probably not capable of very strong play.
Before we dive into the details of an improved model architecture, it would be very helpful
to have a decent, fixed benchmark to gauge our progress.&lt;/p&gt;
&lt;h2 id=&#34;a-benchmark-opponent&#34;&gt;A benchmark opponent&lt;/h2&gt;
&lt;p&gt;The only model with fixed performance we have right now is the &lt;code&gt;RandomPlayer&lt;/code&gt; from the
&lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;basic setup post&lt;/a&gt;. Obviously, that&amp;rsquo;s not a challenging
bar to clear. But it turns out that with some small tweaks, we can turn the fully random
player into a formidable opponent for our starter models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Entropy Regularization</title>
      <link>https://c-f-h.github.io/post/entropy-regularization/</link>
      <pubDate>Thu, 24 Apr 2025 20:15:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/entropy-regularization/</guid>
      <description>&lt;p&gt;Based on our &lt;a href=&#34;https://c-f-h.github.io/post/on-entropy/&#34;&gt;discussion on entropy&lt;/a&gt;, our plan is to implement
entropy regularization via an entropy bonus in our loss function.&lt;/p&gt;
&lt;p&gt;Runnable example code for this post is at
&lt;a href=&#34;https://github.com/c-f-h/connect-zero/blob/main/train/example2-entropy.py&#34;&gt;&lt;code&gt;connect-zero/train/example2-entropy.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;implementing-the-entropy-bonus&#34;&gt;Implementing the entropy bonus&lt;/h2&gt;
&lt;p&gt;The formula for entropy which we have to implement,&lt;/p&gt;
\[
    H(p) = -\sum_{i=1}^{C} p_i \log p_i,
\]&lt;p&gt;is simple enough: multiply the probabilities for the seven possible moves with their
log-probabilities, sum and negate. However, there is one numerical problem we
have to worry about: masking out an illegal move \(i\) leads to a zero probability
\(p_i=0\) and a log-probability \(\log p_i = -\infty\). However, due to the rules of
IEEE 754 floating point numbers, multiplying zero with \(\pm\infty\) is undefined and
therefore results in NaN (not a number). For the entropy formula, however, the
contribution should be 0.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On Entropy</title>
      <link>https://c-f-h.github.io/post/on-entropy/</link>
      <pubDate>Wed, 23 Apr 2025 20:57:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/on-entropy/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/policy-collapse/&#34;&gt;The last time&lt;/a&gt;, we ran our first self-play training loop
on a simple MLP model and observed catastrophic policy collapse. Let&amp;rsquo;s first understand
some of the math behind what happened, and then how to combat it.&lt;/p&gt;
&lt;h2 id=&#34;what-is-entropy&#34;&gt;What is entropy?&lt;/h2&gt;
&lt;p&gt;Given a probability distribution \(p=(p_1,\ldots,p_C)\)
over a number of categories \(i=1,\ldots,C\), such as the distribution over the columns our
Connect 4 model outputs for a given board state, entropy measures the &amp;ldquo;amount of
randomness&amp;rdquo; and is defined as&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>A First Training Run and Policy Collapse</title>
      <link>https://c-f-h.github.io/post/policy-collapse/</link>
      <pubDate>Mon, 21 Apr 2025 17:45:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/policy-collapse/</guid>
      <description>&lt;p&gt;With the &lt;a href=&#34;https://c-f-h.github.io/post/the-reinforce-algorithm/&#34;&gt;REINFORCE algorithm&lt;/a&gt; under our belt,
we can finally attempt to start training some models for
&lt;a href=&#34;https://c-f-h.github.io/post/connect-zero/&#34;&gt;Connect 4&lt;/a&gt;.
However, as we&amp;rsquo;ll see, there are still some hurdles in our way before we get anywhere.
It&amp;rsquo;s good to set your expectations accordingly because rarely if ever do things go
smoothly the first time in RL.&lt;/p&gt;
&lt;p&gt;Runnable example code for this post is at
&lt;a href=&#34;https://github.com/c-f-h/connect-zero/blob/main/train/example1-collapse.py&#34;&gt;&lt;code&gt;connect-zero/train/example1-collapse.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;a-simple-mlp-model&#34;&gt;A simple MLP model&lt;/h2&gt;
&lt;p&gt;As a fruitfly of Connect 4-playing models, let&amp;rsquo;s start with a simple multilayer perceptron
(MLP) model that follows the &lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;model protocol&lt;/a&gt; we
outlined earlier: that means that it has an input layer taking a 6x7 &lt;code&gt;int8&lt;/code&gt; board state
tensor, a few simple hidden layers consisting of just a linear layer and a ReLU activation
function each, and an output layer of 7 neurons without any activation function&amp;mdash;that&amp;rsquo;s
exactly what we meant earlier when we said that the model should output raw logits.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The REINFORCE Algorithm</title>
      <link>https://c-f-h.github.io/post/the-reinforce-algorithm/</link>
      <pubDate>Sun, 20 Apr 2025 20:29:21 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/the-reinforce-algorithm/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s say we have a Connect 4-playing model and we let it
&lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;play a couple of games&lt;/a&gt;.
(We haven&amp;rsquo;t really talked about model architecture until now, so for now just imagine a
simple multilayer perceptron with a few hidden layers which outputs 7 raw logits,
as discussed in the previous post.)&lt;/p&gt;
&lt;p&gt;As it goes in life, our model wins some and loses some. How do we make it
actually learn from its experiences? How does the magic happen?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Connect-Zero: Reinforcement Learning from Scratch</title>
      <link>https://c-f-h.github.io/post/connect-zero/</link>
      <pubDate>Sun, 20 Apr 2025 13:12:41 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/connect-zero/</guid>
      <description>&lt;p&gt;For a long time I&amp;rsquo;ve wanted to get deeper into reinforcement learning (RL), and the project
I finally settled on is teaching a neural network model
how to play the classic game &lt;strong&gt;Connect 4&lt;/strong&gt; (&lt;a href=&#34;https://www.youtube.com/watch?v=KN3nohBw_CE&#34;&gt;pretty sneaky, sis!&lt;/a&gt;).
Obviously, the name &amp;ldquo;Connect-Zero&amp;rdquo; is a cheeky nod to AlphaGo Zero
and AlphaZero by DeepMind.
I chose Connect 4 because it&amp;rsquo;s a simple game everyone knows how to play where we can
hope to achieve good results without expensive hardware and high training costs.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
