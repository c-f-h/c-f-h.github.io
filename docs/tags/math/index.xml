<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Math on cfh::blog</title>
    <link>https://c-f-h.github.io/tags/math/</link>
    <description>Recent content in Math on cfh::blog</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Apr 2025 08:42:00 +0200</lastBuildDate>
    <atom:link href="https://c-f-h.github.io/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>REINFORCE with Baseline</title>
      <link>https://c-f-h.github.io/post/reinforce-with-baseline/</link>
      <pubDate>Tue, 29 Apr 2025 08:42:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/reinforce-with-baseline/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/model-design/&#34;&gt;In the previous post&lt;/a&gt;, we introduced a stronger model
but observed that it&amp;rsquo;s quite challenging to achieve
a high level of play with &lt;a href=&#34;https://c-f-h.github.io/post/the-reinforce-algorithm/&#34;&gt;basic REINFORCE&lt;/a&gt;,
due to the high variance and noisy gradients of the algorithm which often lead to unstable
learning and slow convergence.
Our first step towards more advanced algorithms is a modification called
&amp;ldquo;REINFORCE with baseline&amp;rdquo; (see, e.g.,
&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&#34;&gt;Sutton et al. (2000)&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;the-value-network&#34;&gt;The value network&lt;/h2&gt;
&lt;p&gt;Given a board state \(s\), &lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;recall that&lt;/a&gt; our model
currently outputs seven raw logits which are then
transformed via softmax into the probability distribution \(p(s)\) over the seven possible
moves. Many advanced algorithms in RL assume that our network also outputs
a second piece of information: the &lt;strong&gt;value&lt;/strong&gt; \(v(s)\), a number between -1 and 1 which,
roughly speaking, gives an estimate of how confident the model is in winning from the
current position.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On Entropy</title>
      <link>https://c-f-h.github.io/post/on-entropy/</link>
      <pubDate>Wed, 23 Apr 2025 20:57:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/on-entropy/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/policy-collapse/&#34;&gt;The last time&lt;/a&gt;, we ran our first self-play training loop
on a simple MLP model and observed catastrophic policy collapse. Let&amp;rsquo;s first understand
some of the math behind what happened, and then how to combat it.&lt;/p&gt;
&lt;h2 id=&#34;what-is-entropy&#34;&gt;What is entropy?&lt;/h2&gt;
&lt;p&gt;Given a probability distribution \(p=(p_1,\ldots,p_C)\)
over a number of categories \(i=1,\ldots,C\), such as the distribution over the columns our
Connect 4 model outputs for a given board state, entropy measures the &amp;ldquo;amount of
randomness&amp;rdquo; and is defined as&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>A First Training Run and Policy Collapse</title>
      <link>https://c-f-h.github.io/post/policy-collapse/</link>
      <pubDate>Mon, 21 Apr 2025 17:45:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/policy-collapse/</guid>
      <description>&lt;p&gt;With the &lt;a href=&#34;https://c-f-h.github.io/post/the-reinforce-algorithm/&#34;&gt;REINFORCE algorithm&lt;/a&gt; under our belt,
we can finally attempt to start training some models for
&lt;a href=&#34;https://c-f-h.github.io/post/connect-zero/&#34;&gt;Connect 4&lt;/a&gt;.
However, as we&amp;rsquo;ll see, there are still some hurdles in our way before we get anywhere.
It&amp;rsquo;s good to set your expectations accordingly because rarely if ever do things go
smoothly the first time in RL.&lt;/p&gt;

  
  
    &lt;blockquote class=&#34;alert alert-important&#34;&gt;
      &lt;p class=&#34;alert-heading&#34;&gt;
        ‚ùï
        
          &lt;b&gt;Example Code&lt;/b&gt;
        
      &lt;/p&gt;
      &lt;p&gt;Runnable example code for this post:&lt;br&gt;
&lt;a href=&#34;https://github.com/c-f-h/connect-zero/blob/main/train/example1-collapse.py&#34;&gt;&lt;code&gt;connect-zero/train/example1-collapse.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;h2 id=&#34;a-simple-mlp-model&#34;&gt;A simple MLP model&lt;/h2&gt;
&lt;p&gt;As a fruitfly of Connect 4-playing models, let&amp;rsquo;s start with a simple multilayer perceptron
(MLP) model that follows the &lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;model protocol&lt;/a&gt; we
outlined earlier: that means that it has an input layer taking a 6x7 &lt;code&gt;int8&lt;/code&gt; board state
tensor, a few simple hidden layers consisting of just a linear layer and a ReLU activation
function each, and an output layer of 7 neurons without any activation function&amp;mdash;that&amp;rsquo;s
exactly what we meant earlier when we said that the model should output raw logits.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The REINFORCE Algorithm</title>
      <link>https://c-f-h.github.io/post/the-reinforce-algorithm/</link>
      <pubDate>Sun, 20 Apr 2025 20:29:21 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/the-reinforce-algorithm/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s say we have a Connect 4-playing model and we let it
&lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;play a couple of games&lt;/a&gt;.
(We haven&amp;rsquo;t really talked about model architecture until now, so for now just imagine a
simple multilayer perceptron with a few hidden layers which outputs 7 raw logits,
as discussed in the previous post.)&lt;/p&gt;
&lt;p&gt;As it goes in life, our model wins some and loses some. How do we make it
actually learn from its experiences? How does the magic happen?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Preparing the Data</title>
      <link>https://c-f-h.github.io/post/preparing-the-data/</link>
      <pubDate>Wed, 09 Apr 2025 20:57:38 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/preparing-the-data/</guid>
      <description>&lt;p&gt;With the &lt;a href=&#34;https://c-f-h.github.io/post/refining-intersections/&#34;&gt;triangle self-intersection algorithm&lt;/a&gt; ready to go, we
can start gathering the training data for our machine learning setup. But first we have to think about how
exactly we want to represent it.&lt;/p&gt;
&lt;h2 id=&#34;canonical-triangles&#34;&gt;Canonical triangles&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://c-f-h.github.io/post/the-triangle-problem/&#34;&gt;curved triangles&lt;/a&gt; we work with are specified by six 3D vectors, so that
would mean 18 floating point numbers as our input data.
But an important insight is that whether a triangle intersects itself doesn&amp;rsquo;t change when we rotate it, translate it,
or uniformly scale it&amp;mdash;it&amp;rsquo;s well known that affine transformations of spline control points result in affine transformations of the surface itself.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Getting Accurate Intersections with Gauss-Newton</title>
      <link>https://c-f-h.github.io/post/refining-intersections/</link>
      <pubDate>Tue, 08 Apr 2025 08:33:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/refining-intersections/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;https://c-f-h.github.io/post/computing-intersections/&#34;&gt;last post&lt;/a&gt;, we found pairs of subtriangles of our
&lt;a href=&#34;https://c-f-h.github.io/post/the-triangle-problem/&#34;&gt;curved triangle&lt;/a&gt; which intersect.
The subtriangles were linear approximations, which means that the intersection points we found are also only approximate.
This might be good enough for our purposes, but in the interest of getting training data that&amp;rsquo;s as accurate as possible, we will
refine these intersections by projecting them onto the exact curved triangle.&lt;/p&gt;
&lt;p&gt;To be precise, we are looking for two distinct parameter pairs \((u_1, v_1)\) and \((u_2, v_2)\) within the triangle&amp;rsquo;s domain such that their mappings coincide,&lt;/p&gt;</description>
    </item>
    <item>
      <title>Computing Self-Intersections, the Geometric Way</title>
      <link>https://c-f-h.github.io/post/computing-intersections/</link>
      <pubDate>Mon, 07 Apr 2025 17:52:07 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/computing-intersections/</guid>
      <description>&lt;p&gt;Before we can apply ML to the &lt;a href=&#34;https://c-f-h.github.io/post/the-triangle-problem/&#34;&gt;triangle problem&lt;/a&gt;, we need to be able to
compute self-intersections of a curved triangle in an accurate and efficient way so that we can generate enough training data.&lt;/p&gt;
&lt;p&gt;The basic approach is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#subdividing-the-triangle&#34;&gt;Subdivide&lt;/a&gt; the curved triangle into smaller subtriangles&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#finding-candidate-pairs-using-a-bounding-volume-hierarchy-bvh&#34;&gt;Find&lt;/a&gt; potentially intersecting pairs of subtriangles&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#finding-true-intersection-pairs&#34;&gt;Check&lt;/a&gt; for actual intersections among these candidate pairs&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;subdividing-the-triangle&#34;&gt;Subdividing the triangle&lt;/h2&gt;
&lt;p&gt;We split the original triangle into a list of sufficiently flat subtriangles by a simple recursive
procedure, starting with the full triangle &lt;code&gt;{(0,0), (1,0), (0,1)}&lt;/code&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Curved Triangle Problem</title>
      <link>https://c-f-h.github.io/post/the-triangle-problem/</link>
      <pubDate>Sun, 06 Apr 2025 21:06:14 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/the-triangle-problem/</guid>
      <description>&lt;p&gt;As the starting point for a little machine learning project, I chose the following geometric problem.
We are given a curved triangle in 3D space.
It&amp;rsquo;s specified via its three vertices plus three additional vector-valued coefficients associated to its three edges.
These coefficients are interpreted as control points of a quadratic triangular Bezier surface.
Such representations are commonly used in CAD systems to represent curved surfaces.
Mathematically speaking, we map parameters \((u,v)\) which lie in the parameter-space triangle \( 0 \le u, v;\ u+v \le 1\) to&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
