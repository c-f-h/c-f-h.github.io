<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Connect-Zero on cfh::blog</title>
    <link>https://c-f-h.github.io/categories/connect-zero/</link>
    <description>Recent content in Connect-Zero on cfh::blog</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 May 2025 21:56:00 +0200</lastBuildDate>
    <atom:link href="https://c-f-h.github.io/categories/connect-zero/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Actor-Critic Algorithms</title>
      <link>https://c-f-h.github.io/post/actor-critic/</link>
      <pubDate>Thu, 08 May 2025 21:56:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/actor-critic/</guid>
      <description>&lt;p&gt;After &lt;a href=&#34;https://c-f-h.github.io/post/implementing-rwb/&#34;&gt;implementing and evaluating&lt;/a&gt; REINFORCE with baseline,
we found that it can produce strong models, but takes a long time to learn an accurate
value function due to the high variance of the
&lt;a href=&#34;https://c-f-h.github.io/post/reinforce-with-baseline/#monte-carlo-sampling&#34;&gt;Monte Carlo samples&lt;/a&gt;.
In this post, we&amp;rsquo;ll look at Actor-Critic methods, and in particular the
Advantage Actor-Critic (A2C) algorithm&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;,
a synchronous version of the earlier Asynchronous Advantage Actor-Critic (A3C) method,
as a way to remedy this.&lt;/p&gt;
&lt;p&gt;Before we start, recall that we introduced a &lt;a href=&#34;https://c-f-h.github.io/post/reinforce-with-baseline/#the-value-network&#34;&gt;value network&lt;/a&gt; as a component of our model; this remains the same for A2C, and in fact
we don&amp;rsquo;t need to modify the network architecture at all to use this newer algorithm.
Our model still consists of a residual CNN backbone, a policy head and a value head.
The value head serves as the &amp;ldquo;critic,&amp;rdquo; whereas the policy head is the &amp;ldquo;actor&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Implementing and Evaluating REINFORCE with Baseline</title>
      <link>https://c-f-h.github.io/post/implementing-rwb/</link>
      <pubDate>Thu, 01 May 2025 23:05:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/implementing-rwb/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/reinforce-with-baseline/&#34;&gt;Having introduced&lt;/a&gt; REINFORCE with baseline on a
conceptual level, let&amp;rsquo;s implement it for our Connect 4-playing CNN model.&lt;/p&gt;
&lt;div class=&#34;example-code&#34;&gt;
  &lt;div class=&#34;header&#34;&gt;
    &lt;svg viewBox=&#34;0 0 16 16&#34; fill=&#34;none&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
  &lt;path d=&#34;M5.79 1.574h3.866c.14 0 .252.11.252.246v5.186a.25.25 0 01-.252.246H6.344c-.975 0-1.766.77-1.766 1.72v1.162a.25.25 0 01-.253.243H1.867a.25.25 0 01-.253-.246V6.177a.25.25 0 01.252-.246H7.98c.418 0 .757-.33.757-.737a.747.747 0 00-.757-.738H5.537V1.82a.25.25 0 01.253-.246zm5.632 2.592V1.82c0-.95-.79-1.72-1.766-1.72H5.79c-.976 0-1.767.77-1.767 1.72v2.636H1.867C.89 4.456.1 5.226.1 6.176v3.955c0 .95.79 1.72 1.766 1.72h2.46c.085 0 .17-.006.252-.017v2.346c0 .95.79 1.72 1.766 1.72h3.866c.976 0 1.767-.77 1.767-1.72v-2.636h2.156c.976 0 1.767-.77 1.767-1.72V5.868c0-.95-.79-1.72-1.767-1.72h-2.458c-.086 0-.17.005-.253.017zm-5.33 5.974V8.994a.25.25 0 01.252-.246h3.312c.976 0 1.766-.77 1.766-1.72V5.866a.25.25 0 01.253-.243h2.458c.14 0 .253.11.253.246v3.954a.25.25 0 01-.252.246H8.02a.747.747 0 00-.757.737c0 .408.339.738.757.738h2.442v2.636a.25.25 0 01-.253.246H6.344a.25.25 0 01-.252-.246v-4.04z&#34; fill=&#34;currentColor&#34;/&gt;
&lt;/svg&gt;
    &lt;span&gt;Runnable Example&lt;/span&gt;&lt;/div&gt;
  &lt;a href=&#34;https://github.com/c-f-h/connect-zero/blob/main/train/example3-rwb.py&#34; target=&#34;_blank&#34;&gt;
    connect-zero/train/example3-rwb.py
  &lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;adding-the-value-head&#34;&gt;Adding the value head&lt;/h2&gt;
&lt;p&gt;In the constructor of the &lt;a href=&#34;https://c-f-h.github.io/post/model-design/&#34;&gt;&lt;code&gt;Connect4CNN&lt;/code&gt; model class&lt;/a&gt;,
we set up the new network for estimating the board state value \(v(s)\)
which will consume the same 448 downsampled features that the policy head receives:&lt;/p&gt;</description>
    </item>
    <item>
      <title>REINFORCE with Baseline</title>
      <link>https://c-f-h.github.io/post/reinforce-with-baseline/</link>
      <pubDate>Tue, 29 Apr 2025 08:42:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/reinforce-with-baseline/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/model-design/&#34;&gt;In the previous post&lt;/a&gt;, we introduced a stronger model
but observed that it&amp;rsquo;s quite challenging to achieve
a high level of play with &lt;a href=&#34;https://c-f-h.github.io/post/the-reinforce-algorithm/&#34;&gt;basic REINFORCE&lt;/a&gt;,
due to the high variance and noisy gradients of the algorithm which often lead to unstable
learning and slow convergence.
Our first step towards more advanced algorithms is a modification called
&amp;ldquo;REINFORCE with baseline&amp;rdquo; (see, e.g.,
&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&#34;&gt;Sutton et al. (2000)&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;the-value-network&#34;&gt;The value network&lt;/h2&gt;
&lt;p&gt;Given a board state \(s\), &lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;recall that&lt;/a&gt; our model
currently outputs seven raw logits which are then
transformed via softmax into the probability distribution \(p(s)\) over the seven possible
moves. Many advanced algorithms in RL assume that our network also outputs
a second piece of information: the &lt;strong&gt;value&lt;/strong&gt; \(v(s)\), a number between -1 and 1 which,
roughly speaking, gives an estimate of how confident the model is in winning from the
current position.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model Design for Connect 4</title>
      <link>https://c-f-h.github.io/post/model-design/</link>
      <pubDate>Mon, 28 Apr 2025 22:30:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/model-design/</guid>
      <description>&lt;p&gt;With the &lt;a href=&#34;https://c-f-h.github.io/post/random-punisher/&#34;&gt;fearsome RandomPunisher&lt;/a&gt; putting our
&lt;a href=&#34;https://c-f-h.github.io/post/entropy-regularization/&#34;&gt;first Connect 4 toy model&lt;/a&gt;
in its place, it&amp;rsquo;s time to design something that stands a chance.&lt;/p&gt;
&lt;h2 id=&#34;a-design-based-on-cnns&#34;&gt;A design based on CNNs&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s standard practice for board-game playing neural networks to have at least a few
&lt;strong&gt;convolutional neural network (CNN)&lt;/strong&gt; layers at the initial inputs. This shouldn&amp;rsquo;t come as
a surprise: the board is a regular grid, much like an image, and CNNs are strong performers
in image processing. In our case, it will allow the model to learn features like
&amp;ldquo;here are three of my pieces in a diagonal downward row&amp;rdquo; which are then automatically
applied to every position on the board, rather than having to re-learn these features
individually at each board position.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introducing a Benchmark Opponent</title>
      <link>https://c-f-h.github.io/post/random-punisher/</link>
      <pubDate>Sat, 26 Apr 2025 09:50:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/random-punisher/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/entropy-regularization/&#34;&gt;Last time&lt;/a&gt; we saw how the entropy bonus
enables self-play training without running into policy collapse.
However, the model we trained was quite small and probably not capable of very strong play.
Before we dive into the details of an improved model architecture, it would be very helpful
to have a decent, fixed benchmark to gauge our progress.&lt;/p&gt;
&lt;h2 id=&#34;a-benchmark-opponent&#34;&gt;A benchmark opponent&lt;/h2&gt;
&lt;p&gt;The only model with fixed performance we have right now is the &lt;code&gt;RandomPlayer&lt;/code&gt; from the
&lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;basic setup post&lt;/a&gt;. Obviously, that&amp;rsquo;s not a challenging
bar to clear. But it turns out that with some small tweaks, we can turn the fully random
player into a formidable opponent for our starter models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Entropy Regularization</title>
      <link>https://c-f-h.github.io/post/entropy-regularization/</link>
      <pubDate>Thu, 24 Apr 2025 20:15:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/entropy-regularization/</guid>
      <description>&lt;p&gt;Based on our &lt;a href=&#34;https://c-f-h.github.io/post/on-entropy/&#34;&gt;discussion on entropy&lt;/a&gt;, our plan is to implement
entropy regularization via an entropy bonus in our loss function.&lt;/p&gt;
&lt;div class=&#34;example-code&#34;&gt;
  &lt;div class=&#34;header&#34;&gt;
    &lt;svg viewBox=&#34;0 0 16 16&#34; fill=&#34;none&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
  &lt;path d=&#34;M5.79 1.574h3.866c.14 0 .252.11.252.246v5.186a.25.25 0 01-.252.246H6.344c-.975 0-1.766.77-1.766 1.72v1.162a.25.25 0 01-.253.243H1.867a.25.25 0 01-.253-.246V6.177a.25.25 0 01.252-.246H7.98c.418 0 .757-.33.757-.737a.747.747 0 00-.757-.738H5.537V1.82a.25.25 0 01.253-.246zm5.632 2.592V1.82c0-.95-.79-1.72-1.766-1.72H5.79c-.976 0-1.767.77-1.767 1.72v2.636H1.867C.89 4.456.1 5.226.1 6.176v3.955c0 .95.79 1.72 1.766 1.72h2.46c.085 0 .17-.006.252-.017v2.346c0 .95.79 1.72 1.766 1.72h3.866c.976 0 1.767-.77 1.767-1.72v-2.636h2.156c.976 0 1.767-.77 1.767-1.72V5.868c0-.95-.79-1.72-1.767-1.72h-2.458c-.086 0-.17.005-.253.017zm-5.33 5.974V8.994a.25.25 0 01.252-.246h3.312c.976 0 1.766-.77 1.766-1.72V5.866a.25.25 0 01.253-.243h2.458c.14 0 .253.11.253.246v3.954a.25.25 0 01-.252.246H8.02a.747.747 0 00-.757.737c0 .408.339.738.757.738h2.442v2.636a.25.25 0 01-.253.246H6.344a.25.25 0 01-.252-.246v-4.04z&#34; fill=&#34;currentColor&#34;/&gt;
&lt;/svg&gt;
    &lt;span&gt;Runnable Example&lt;/span&gt;&lt;/div&gt;
  &lt;a href=&#34;https://github.com/c-f-h/connect-zero/blob/main/train/example2-entropy.py&#34; target=&#34;_blank&#34;&gt;
    connect-zero/train/example2-entropy.py
  &lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;implementing-the-entropy-bonus&#34;&gt;Implementing the entropy bonus&lt;/h2&gt;
&lt;p&gt;The formula for entropy which we have to implement,&lt;/p&gt;</description>
    </item>
    <item>
      <title>On Entropy</title>
      <link>https://c-f-h.github.io/post/on-entropy/</link>
      <pubDate>Wed, 23 Apr 2025 20:57:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/on-entropy/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://c-f-h.github.io/post/policy-collapse/&#34;&gt;The last time&lt;/a&gt;, we ran our first self-play training loop
on a simple MLP model and observed catastrophic policy collapse. Let&amp;rsquo;s first understand
some of the math behind what happened, and then how to combat it.&lt;/p&gt;
&lt;h2 id=&#34;what-is-entropy&#34;&gt;What is entropy?&lt;/h2&gt;
&lt;p&gt;Given a probability distribution \(p=(p_1,\ldots,p_C)\)
over a number of categories \(i=1,\ldots,C\), such as the distribution over the columns our
Connect 4 model outputs for a given board state, entropy measures the &amp;ldquo;amount of
randomness&amp;rdquo; and is defined as&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>A First Training Run and Policy Collapse</title>
      <link>https://c-f-h.github.io/post/policy-collapse/</link>
      <pubDate>Mon, 21 Apr 2025 17:45:00 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/policy-collapse/</guid>
      <description>&lt;p&gt;With the &lt;a href=&#34;https://c-f-h.github.io/post/the-reinforce-algorithm/&#34;&gt;REINFORCE algorithm&lt;/a&gt; under our belt,
we can finally attempt to start training some models for
&lt;a href=&#34;https://c-f-h.github.io/post/connect-zero/&#34;&gt;Connect 4&lt;/a&gt;.
However, as we&amp;rsquo;ll see, there are still some hurdles in our way before we get anywhere.
It&amp;rsquo;s good to set your expectations accordingly because rarely if ever do things go
smoothly the first time in RL.&lt;/p&gt;
&lt;div class=&#34;example-code&#34;&gt;
  &lt;div class=&#34;header&#34;&gt;
    &lt;svg viewBox=&#34;0 0 16 16&#34; fill=&#34;none&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
  &lt;path d=&#34;M5.79 1.574h3.866c.14 0 .252.11.252.246v5.186a.25.25 0 01-.252.246H6.344c-.975 0-1.766.77-1.766 1.72v1.162a.25.25 0 01-.253.243H1.867a.25.25 0 01-.253-.246V6.177a.25.25 0 01.252-.246H7.98c.418 0 .757-.33.757-.737a.747.747 0 00-.757-.738H5.537V1.82a.25.25 0 01.253-.246zm5.632 2.592V1.82c0-.95-.79-1.72-1.766-1.72H5.79c-.976 0-1.767.77-1.767 1.72v2.636H1.867C.89 4.456.1 5.226.1 6.176v3.955c0 .95.79 1.72 1.766 1.72h2.46c.085 0 .17-.006.252-.017v2.346c0 .95.79 1.72 1.766 1.72h3.866c.976 0 1.767-.77 1.767-1.72v-2.636h2.156c.976 0 1.767-.77 1.767-1.72V5.868c0-.95-.79-1.72-1.767-1.72h-2.458c-.086 0-.17.005-.253.017zm-5.33 5.974V8.994a.25.25 0 01.252-.246h3.312c.976 0 1.766-.77 1.766-1.72V5.866a.25.25 0 01.253-.243h2.458c.14 0 .253.11.253.246v3.954a.25.25 0 01-.252.246H8.02a.747.747 0 00-.757.737c0 .408.339.738.757.738h2.442v2.636a.25.25 0 01-.253.246H6.344a.25.25 0 01-.252-.246v-4.04z&#34; fill=&#34;currentColor&#34;/&gt;
&lt;/svg&gt;
    &lt;span&gt;Runnable Example&lt;/span&gt;&lt;/div&gt;
  &lt;a href=&#34;https://github.com/c-f-h/connect-zero/blob/main/train/example1-collapse.py&#34; target=&#34;_blank&#34;&gt;
    connect-zero/train/example1-collapse.py
  &lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;a-simple-mlp-model&#34;&gt;A simple MLP model&lt;/h2&gt;
&lt;p&gt;As a fruitfly of Connect 4-playing models, let&amp;rsquo;s start with a simple multilayer perceptron
(MLP) model that follows the &lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;model protocol&lt;/a&gt; we
outlined earlier: that means that it has an input layer taking a 6x7 &lt;code&gt;int8&lt;/code&gt; board state
tensor, a few simple hidden layers consisting of just a linear layer and a ReLU activation
function each, and an output layer of 7 neurons without any activation function&amp;mdash;that&amp;rsquo;s
exactly what we meant earlier when we said that the model should output raw logits.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The REINFORCE Algorithm</title>
      <link>https://c-f-h.github.io/post/the-reinforce-algorithm/</link>
      <pubDate>Sun, 20 Apr 2025 20:29:21 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/the-reinforce-algorithm/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s say we have a Connect 4-playing model and we let it
&lt;a href=&#34;https://c-f-h.github.io/post/basic-setup-and-play/&#34;&gt;play a couple of games&lt;/a&gt;.
(We haven&amp;rsquo;t really talked about model architecture until now, so for now just imagine a
simple multilayer perceptron with a few hidden layers which outputs 7 raw logits,
as discussed in the previous post.)&lt;/p&gt;
&lt;p&gt;As it goes in life, our model wins some and loses some. How do we make it
actually learn from its experiences? How does the magic happen?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Basic Setup and Play</title>
      <link>https://c-f-h.github.io/post/basic-setup-and-play/</link>
      <pubDate>Sun, 20 Apr 2025 15:35:41 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/basic-setup-and-play/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s get into a bit more technical detail on how our
&lt;a href=&#34;https://c-f-h.github.io/post/connect-zero/&#34;&gt;Connect 4-playing model&lt;/a&gt;
will be set up, and how a basic game loop works.
Throughout all code samples we&amp;rsquo;ll always assume the standard PyTorch imports:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.nn&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;nn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.nn.functional&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;F&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;board-state&#34;&gt;Board state&lt;/h2&gt;
&lt;p&gt;The current board state will be represented by a 6x7 PyTorch &lt;code&gt;int8&lt;/code&gt; tensor,
initially filled with zeros.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;board&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ROWS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;COLS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;int8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DEVICE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The board is ordered such that &lt;code&gt;board[0, :]&lt;/code&gt; is the top row.
A non-empty cell is represented by +1 or -1. To simplify things, we always
represent the player whose move it currently is by +1, and the opponent by -1.
This way we don&amp;rsquo;t need any separate state to keep track of whose move it is.
After a move has been made, we simply flip the board by doing&lt;/p&gt;</description>
    </item>
    <item>
      <title>Connect-Zero: Reinforcement Learning from Scratch</title>
      <link>https://c-f-h.github.io/post/connect-zero/</link>
      <pubDate>Sun, 20 Apr 2025 13:12:41 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/connect-zero/</guid>
      <description>&lt;p&gt;For a long time I&amp;rsquo;ve wanted to get deeper into reinforcement learning (RL), and the project
I finally settled on is teaching a neural network model
how to play the classic game &lt;strong&gt;Connect 4&lt;/strong&gt; (&lt;a href=&#34;https://www.youtube.com/watch?v=KN3nohBw_CE&#34;&gt;pretty sneaky, sis!&lt;/a&gt;).
Obviously, the name &amp;ldquo;Connect-Zero&amp;rdquo; is a cheeky nod to AlphaGo Zero
and AlphaZero by DeepMind.
I chose Connect 4 because it&amp;rsquo;s a simple game everyone knows how to play where we can
hope to achieve good results without expensive hardware and high training costs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Connect 4</title>
      <link>https://c-f-h.github.io/post/connect-4/</link>
      <pubDate>Sun, 20 Apr 2025 09:29:36 +0200</pubDate>
      <guid>https://c-f-h.github.io/post/connect-4/</guid>
      <description>&lt;div id=&#34;game-container1&#34; class=&#34;connect4-container&#34;
    data-human=&#34;1&#34; data-cpu=&#34;2&#34;
    data-random-first-player=&#34;true&#34;
    data-onnx-model=&#34;export.onnx&#34;
&gt;&lt;/div&gt;
&lt;p&gt;The computer opponent is a neural network trained using reinforcement learning.
It was exported to ONNX and now runs right here in your browser.
See &lt;a href=&#34;https://c-f-h.github.io/post/connect-zero/&#34;&gt;Connect-Zero&lt;/a&gt; and the follow-up posts for details.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
